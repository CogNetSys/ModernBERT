{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9r+YRl5ttqRQ+bI0ToiQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df169e2a7af04432a088c67beb8c3782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b4053e0b86844669d62a568a175f3d4",
              "IPY_MODEL_18ad30c72480430fa1965ed8b5f599b5",
              "IPY_MODEL_155bae3009e444a282085f8598991dcb"
            ],
            "layout": "IPY_MODEL_71911bb906fd4c9690f14cbe2ce80be9"
          }
        },
        "4b4053e0b86844669d62a568a175f3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d85c19c0a014fe2bcacfdb629545bf4",
            "placeholder": "​",
            "style": "IPY_MODEL_0cf11862a7864af386b4de48bedc0ebb",
            "value": "model.safetensors: 100%"
          }
        },
        "18ad30c72480430fa1965ed8b5f599b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71df83a44914ae091b8c5049a0ef747",
            "max": 1583544840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08e5f36f4dad4f658dfcfa9d7f9ea17e",
            "value": 1583544840
          }
        },
        "155bae3009e444a282085f8598991dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8e045cef124b8c844fb93e6d5bad46",
            "placeholder": "​",
            "style": "IPY_MODEL_5d202ccf4c084ab2b05092fdeaf55417",
            "value": " 1.58G/1.58G [00:37&lt;00:00, 35.5MB/s]"
          }
        },
        "71911bb906fd4c9690f14cbe2ce80be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d85c19c0a014fe2bcacfdb629545bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cf11862a7864af386b4de48bedc0ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a71df83a44914ae091b8c5049a0ef747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e5f36f4dad4f658dfcfa9d7f9ea17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a8e045cef124b8c844fb93e6d5bad46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d202ccf4c084ab2b05092fdeaf55417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CogNetSys/ModernBERT/blob/main/ModernBERT_Experiment_12_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-M7yO3VKDKT",
        "outputId": "2b46fe39-99f0-4a5c-cac6-c5253b26c8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.48.0 in /usr/local/lib/python3.11/dist-packages (4.48.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2024.12.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install Hugging Face Transformers and other dependencies\n",
        "!pip install transformers==4.48.0\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install seaborn\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate installation\n",
        "!python3 -m spacy validate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EF2fUCXKWvO",
        "outputId": "cedd5296-2af1-4ed5-d5fb-f4ddb655728d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r⠙ Loading compatibility table...\r\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
            "\u001b[1m\n",
            "================= Installed pipeline packages (spaCy v3.7.5) =================\u001b[0m\n",
            "\u001b[38;5;4mℹ spaCy installation: /usr/local/lib/python3.11/dist-packages/spacy\u001b[0m\n",
            "\n",
            "NAME             SPACY            VERSION                            \n",
            "en_core_web_sm   >=3.7.2,<3.8.0   \u001b[38;5;2m3.7.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoKUpGExKPAQ",
        "outputId": "e0fab697-5c72-4c3f-c722-de8d8199141c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define comprehensive entity lists\n",
        "persons = [\n",
        "    \"John Doe\", \"Alice Smith\", \"Maria Garcia\", \"Bob Johnson\", \"Charlie Lee\",\n",
        "    \"David Brown\", \"Emma Wilson\", \"Frank Moore\", \"Grace Taylor\", \"Henry Anderson\"\n",
        "]\n",
        "aliases = [\n",
        "    \"Johnny\", \"Ally\", \"Mia\", \"Bobby\", \"Chuck\",\n",
        "    \"Dave\", \"Em\", \"Frankie\", \"Gracie\", \"Hank\"\n",
        "]\n",
        "titles = [\n",
        "    \"Dr.\", \"Prof.\", \"Mr.\", \"Ms.\", \"Mrs.\",\n",
        "    \"CEO\", \"CTO\", \"Manager\", \"Director\", \"Lead\"\n",
        "]\n",
        "roles = [\n",
        "    \"Software Engineer\", \"Data Scientist\", \"Product Manager\", \"Sales Executive\", \"HR Specialist\",\n",
        "    \"Marketing Coordinator\", \"Financial Analyst\", \"Customer Support Representative\", \"DevOps Engineer\", \"UX Designer\"\n",
        "]\n",
        "organizations = [\n",
        "    \"Acme Corp\", \"Global Tech\", \"Finance Department\", \"HR Team\", \"IT Services\",\n",
        "    \"Research Division\", \"Marketing Department\", \"Sales Team\", \"Operations Unit\", \"Customer Support\"\n",
        "]\n",
        "business_names = organizations.copy()  # Assuming business names align with organizations\n",
        "business_ids = [\n",
        "    \"BUS123456\", \"BUS234567\", \"BUS345678\", \"BUS456789\", \"BUS567890\",\n",
        "    \"BUS678901\", \"BUS789012\", \"BUS890123\", \"BUS901234\", \"BUS012345\"\n",
        "]\n",
        "locations = [\n",
        "    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\",\n",
        "    \"San Francisco\", \"Boston\", \"Seattle\", \"Denver\", \"Miami\"\n",
        "]\n",
        "ips = [\n",
        "    \"192.168.1.1\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.0.100\", \"10.0.1.25\",\n",
        "    \"192.168.1.255\", \"10.0.0.8\", \"172.16.0.45\", \"192.168.0.55\", \"10.0.1.99\"\n",
        "]\n",
        "mac_addresses = [\n",
        "    \"00:1A:2B:3C:4D:5E\", \"11:22:33:44:55:66\", \"AA:BB:CC:DD:EE:FF\",\n",
        "    \"12:34:56:78:9A:BC\", \"DE:F0:12:34:56:78\",\n",
        "    \"98:76:54:32:10:FE\", \"AB:CD:EF:12:34:56\", \"12:AB:34:CD:56:EF\",\n",
        "    \"FE:DC:BA:98:76:54\", \"65:43:21:09:87:65\"\n",
        "]\n",
        "projects = [\n",
        "    \"Project Phoenix\", \"Apollo\", \"Zeus\", \"Hermes\", \"Athena\",\n",
        "    \"Project Titan\", \"Orion\", \"Elysium\", \"Nebula\", \"Vortex\"\n",
        "]\n",
        "dates = [\n",
        "    \"Monday\", \"Tuesday\", \"Wednesday\", \"Saturday\", \"Sunday\",\n",
        "    \"April 5th\", \"July 20th\", \"September 15th\", \"December 1st\", \"January 10th\"\n",
        "]\n",
        "times = [\n",
        "    \"10:00 AM\", \"2:30 PM\", \"5:45 PM\", \"9:15 AM\", \"1:00 PM\",\n",
        "    \"3:20 PM\", \"4:50 PM\", \"11:30 AM\", \"6:00 PM\", \"8:15 PM\"\n",
        "]\n",
        "durations = [\n",
        "    \"2 hours\", \"30 minutes\", \"45 minutes\", \"1 hour\", \"3 hours\",\n",
        "    \"15 minutes\", \"1.5 hours\", \"4 hours\", \"25 minutes\", \"50 minutes\"\n",
        "]\n",
        "events = [\n",
        "    \"Annual Meeting\", \"Quarterly Review\", \"Product Launch\", \"Team Building Retreat\", \"Client Presentation\",\n",
        "    \"Security Audit\", \"System Upgrade\", \"Sales Conference\", \"Marketing Workshop\", \"HR Training\"\n",
        "]\n",
        "emails = [\n",
        "    \"john.doe@example.com\", \"alice.smith@globaltech.com\", \"maria.garcia@acmecorp.com\",\n",
        "    \"bob.johnson@finance.dept.com\", \"charlie.lee@itservices.com\"\n",
        "]\n",
        "phones = [\n",
        "    \"+1-202-555-0156\", \"+1-303-555-0198\", \"+1-404-555-0133\", \"+1-505-555-0177\", \"+1-606-555-0111\",\n",
        "    \"+1-707-555-0144\", \"+1-808-555-0188\", \"+1-909-555-0122\", \"+1-101-555-0166\", \"+1-212-555-0100\"\n",
        "]\n",
        "urls = [\n",
        "    \"https://acme.com/login\", \"https://globaltech.com/dashboard\", \"https://finance.dept.com/reports\",\n",
        "    \"https://hrteam.com/profile\", \"https://itservices.com/support\",\n",
        "    \"https://research.division.com/data\", \"https://marketing.dept.com/campaigns\", \"https://sales.team.com/leads\",\n",
        "    \"https://operations.unit.com/status\", \"https://customersupport.com/help\"\n",
        "]\n",
        "devices = [\n",
        "    \"Laptop-01\", \"Server-12\", \"Router-5\", \"Firewall-3\", \"Workstation-7\",\n",
        "    \"Tablet-4\", \"Smartphone-9\", \"Printer-2\", \"Scanner-6\", \"NAS-8\"\n",
        "]\n",
        "device_ids = devices.copy()  # Assuming device IDs align with devices\n",
        "passwords = [\n",
        "    \"P@ssw0rd!\", \"Secure#123\", \"Admin@2024\", \"User*Pass1\", \"Qwerty!234\",\n",
        "    \"Welcome#1\", \"Passw0rd$\", \"Login*123\", \"MyPass#456\", \"Access@789\"\n",
        "]\n",
        "access_keys = [\n",
        "    \"AK12345XYZ\", \"AK67890ABC\", \"AK54321DEF\", \"AK09876GHI\", \"AK11223JKL\",\n",
        "    \"AK44556MNO\", \"AK77889PQR\", \"AK99000STU\", \"AK13579VWX\", \"AK24680YZA\"\n",
        "]\n",
        "social_security_numbers = [\n",
        "    \"123-45-6789\", \"987-65-4321\", \"555-55-5555\", \"111-22-3333\", \"444-55-6666\",\n",
        "    \"777-88-9999\", \"222-33-4444\", \"333-44-5555\", \"666-77-8888\", \"999-00-1111\"\n",
        "]\n",
        "credit_cards = [\n",
        "    \"4111-1111-1111-1111\", \"5500-0000-0000-0004\", \"3400-0000-0000-009\", \"3000-0000-0000-04\",\n",
        "    \"6011-0000-0000-0004\", \"2014-0000-0000-009\", \"3088-0000-0000-0009\", \"3600-0000-0000-0008\",\n",
        "    \"3800-0000-0000-0028\", \"6304-0000-0000-0003\"\n",
        "]\n",
        "bank_accounts = [\n",
        "    \"BA123456789\", \"BA987654321\", \"BA555555555\", \"BA111222333\", \"BA444555666\",\n",
        "    \"BA777888999\", \"BA000111222\", \"BA333444555\", \"BA666777888\", \"BA999000111\"\n",
        "]\n",
        "license_plates = [\n",
        "    \"ABC-1234\", \"XYZ-5678\", \"LMN-9012\", \"DEF-3456\", \"GHI-7890\",\n",
        "    \"JKL-2345\", \"MNO-6789\", \"PQR-0123\", \"STU-4567\", \"VWX-8901\"\n",
        "]\n",
        "hazmats = [\n",
        "    \"Hazmat Material A\", \"Hazmat Substance B\", \"Hazmat Agent C\", \"Hazmat Compound D\", \"Hazmat Material E\",\n",
        "    \"Hazmat Substance F\", \"Hazmat Agent G\", \"Hazmat Compound H\", \"Hazmat Material I\", \"Hazmat Substance J\"\n",
        "]\n",
        "money = [\n",
        "    \"$1000\", \"$2500\", \"$500\", \"$750\", \"$1200\",\n",
        "    \"$3000\", \"$450\", \"$600\", \"$800\", \"$950\"\n",
        "]\n",
        "currencies = [\n",
        "    \"USD\", \"EUR\", \"GBP\", \"JPY\", \"AUD\",\n",
        "    \"CAD\", \"CHF\", \"CNY\", \"SEK\", \"NZD\"\n",
        "]\n",
        "invoices = [\n",
        "    \"INV1001\", \"INV1002\", \"INV1003\", \"INV1004\", \"INV1005\",\n",
        "    \"INV1006\", \"INV1007\", \"INV1008\", \"INV1009\", \"INV1010\"\n",
        "]\n",
        "transactions = [\n",
        "    \"TXN5001\", \"TXN5002\", \"TXN5003\", \"TXN5004\", \"TXN5005\",\n",
        "    \"TXN5006\", \"TXN5007\", \"TXN5008\", \"TXN5009\", \"TXN5010\"\n",
        "]\n",
        "accounts = [\n",
        "    \"ACCT3001\", \"ACCT3002\", \"ACCT3003\", \"ACCT3004\", \"ACCT3005\",\n",
        "    \"ACCT3006\", \"ACCT3007\", \"ACCT3008\", \"ACCT3009\", \"ACCT3010\"\n",
        "]\n",
        "ticket_ids = [\n",
        "    \"TICKET1001\", \"TICKET1002\", \"TICKET1003\", \"TICKET1004\", \"TICKET1005\",\n",
        "    \"TICKET1006\", \"TICKET1007\", \"TICKET1008\", \"TICKET1009\", \"TICKET1010\"\n",
        "]\n",
        "issue_types = [\n",
        "    \"Login Issue\", \"Password Reset\", \"Account Lockout\", \"Data Breach\", \"System Downtime\",\n",
        "    \"Payment Failure\", \"Feature Request\", \"Bug Report\", \"Access Denied\", \"Performance Lag\"\n",
        "]\n",
        "priorities = [\n",
        "    \"Low\", \"Medium\", \"High\", \"Critical\", \"Urgent\",\n",
        "    \"Low\", \"Medium\", \"High\", \"Critical\", \"Urgent\"\n",
        "]\n",
        "resolution_statuses = [\n",
        "    \"Resolved\", \"Unresolved\", \"In Progress\", \"Pending\", \"Escalated\",\n",
        "    \"Resolved\", \"Unresolved\", \"In Progress\", \"Pending\", \"Escalated\"\n",
        "]\n",
        "leads = [\n",
        "    \"Lead1001\", \"Lead1002\", \"Lead1003\", \"Lead1004\", \"Lead1005\",\n",
        "    \"Lead1006\", \"Lead1007\", \"Lead1008\", \"Lead1009\", \"Lead1010\"\n",
        "]\n",
        "opportunities = [\n",
        "    \"Opp2001\", \"Opp2002\", \"Opp2003\", \"Opp2004\", \"Opp2005\",\n",
        "    \"Opp2006\", \"Opp2007\", \"Opp2008\", \"Opp2009\", \"Opp2010\"\n",
        "]\n",
        "campaigns = [\n",
        "    \"Camp3001\", \"Camp3002\", \"Camp3003\", \"Camp3004\", \"Camp3005\",\n",
        "    \"Camp3006\", \"Camp3007\", \"Camp3008\", \"Camp3009\", \"Camp3010\"\n",
        "]\n",
        "discount_codes = [\n",
        "    \"DISC10\", \"DISC20\", \"DISC30\", \"DISC40\", \"DISC50\",\n",
        "    \"DISC60\", \"DISC70\", \"DISC80\", \"DISC90\", \"DISC100\"\n",
        "]\n",
        "custom1 = [\n",
        "    \"CustomEntity1\", \"CustomEntity2\", \"CustomEntity3\", \"CustomEntity4\", \"CustomEntity5\",\n",
        "    \"CustomEntity6\", \"CustomEntity7\", \"CustomEntity8\", \"CustomEntity9\", \"CustomEntity10\"\n",
        "]\n",
        "custom2 = [\n",
        "    \"CustomEntityA\", \"CustomEntityB\", \"CustomEntityC\", \"CustomEntityD\", \"CustomEntityE\",\n",
        "    \"CustomEntityF\", \"CustomEntityG\", \"CustomEntityH\", \"CustomEntityI\", \"CustomEntityJ\"\n",
        "]\n",
        "username = [\n",
        "    \"user123\", \"admin456\", \"guest789\", \"member012\", \"user345\",\n",
        "    \"admin678\", \"guest901\", \"member234\", \"user567\", \"admin890\"\n",
        "]\n",
        "address_line1 = [\n",
        "    \"123 Maple Street\", \"456 Oak Avenue\", \"789 Pine Road\", \"321 Birch Lane\", \"654 Cedar Blvd\",\n",
        "    \"987 Spruce Drive\", \"213 Elm Street\", \"546 Ash Avenue\", \"879 Fir Road\", \"132 Willow Lane\"\n",
        "]\n",
        "city = locations.copy()  # Assuming city aligns with locations\n",
        "state = [\n",
        "    \"NY\", \"CA\", \"IL\", \"TX\", \"AZ\",\n",
        "    \"MA\", \"WA\", \"CO\", \"FL\", \"NJ\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "u_DebwEoL1Aj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 'O' label\n",
        "O_label = \"O\"\n",
        "\n",
        "# Comprehensive entity_label_map with initial mappings\n",
        "entity_label_map = {\n",
        "    \"person\": {\"B\": \"B-PER\", \"I\": \"I-PER\"},\n",
        "    \"alias\": {\"B\": \"B-ALIAS\", \"I\": \"I-ALIAS\"},\n",
        "    \"title\": {\"B\": \"B-TITLE\", \"I\": \"I-TITLE\"},\n",
        "    \"role\": {\"B\": \"B-ROLE\", \"I\": \"I-ROLE\"},\n",
        "    \"organization\": {\"B\": \"B-ORG\", \"I\": \"I-ORG\"},\n",
        "    \"business_name\": {\"B\": \"B-BUS\", \"I\": \"I-BUS\"},\n",
        "    \"business_id\": {\"B\": \"B-BUSID\", \"I\": \"I-BUSID\"},\n",
        "    \"location\": {\"B\": \"B-LOC\", \"I\": \"I-LOC\"},\n",
        "    \"ip\": {\"B\": \"B-IP\", \"I\": \"I-IP\"},\n",
        "    \"mac_address\": {\"B\": \"B-MAC\", \"I\": \"I-MAC\"},\n",
        "    \"project\": {\"B\": \"B-PROJ\", \"I\": \"I-PROJ\"},\n",
        "    \"date\": {\"B\": \"B-DATE\", \"I\": \"I-DATE\"},\n",
        "    \"time\": {\"B\": \"B-TIME\", \"I\": \"I-TIME\"},\n",
        "    \"duration\": {\"B\": \"B-DUR\", \"I\": \"I-DUR\"},\n",
        "    \"event\": {\"B\": \"B-EVENT\", \"I\": \"I-EVENT\"},\n",
        "    \"email\": {\"B\": \"B-EMAIL\", \"I\": \"I-EMAIL\"},\n",
        "    \"phone\": {\"B\": \"B-PHONE\", \"I\": \"I-PHONE\"},\n",
        "    \"url\": {\"B\": \"B-URL\", \"I\": \"I-URL\"},\n",
        "    \"device\": {\"B\": \"B-DEV\", \"I\": \"I-DEV\"},\n",
        "    \"device_id\": {\"B\": \"B-DEV_ID\", \"I\": \"I-DEV_ID\"},\n",
        "    \"password\": {\"B\": \"B-PASS\", \"I\": \"I-PASS\"},\n",
        "    \"access_key\": {\"B\": \"B-KEY\", \"I\": \"I-KEY\"},\n",
        "    \"social_security\": {\"B\": \"B-SSN\", \"I\": \"I-SSN\"},\n",
        "    \"credit_card\": {\"B\": \"B-CC\", \"I\": \"I-CC\"},\n",
        "    \"bank_account\": {\"B\": \"B-BANK\", \"I\": \"I-BANK\"},\n",
        "    \"license_plate\": {\"B\": \"B-PLATE\", \"I\": \"I-PLATE\"},\n",
        "    \"hazmat\": {\"B\": \"B-HAZMAT\", \"I\": \"I-HAZMAT\"},\n",
        "    \"money\": {\"B\": \"B-MONEY\", \"I\": \"I-MONEY\"},\n",
        "    \"currency\": {\"B\": \"B-CUR\", \"I\": \"I-CUR\"},\n",
        "    \"invoice\": {\"B\": \"B-INVOICE\", \"I\": \"I-INVOICE\"},\n",
        "    \"transaction\": {\"B\": \"B-TRANS\", \"I\": \"I-TRANS\"},\n",
        "    \"account\": {\"B\": \"B-ACCT\", \"I\": \"I-ACCT\"},\n",
        "    \"ticket_id\": {\"B\": \"B-TICKET\", \"I\": \"I-TICKET\"},\n",
        "    \"issue_type\": {\"B\": \"B-ISSUE\", \"I\": \"I-ISSUE\"},\n",
        "    \"priority\": {\"B\": \"B-PRIORITY\", \"I\": \"I-PRIORITY\"},\n",
        "    \"resolution_status\": {\"B\": \"B-STATUS\", \"I\": \"I-STATUS\"},\n",
        "    \"lead\": {\"B\": \"B-LEAD\", \"I\": \"I-LEAD\"},\n",
        "    \"opportunity\": {\"B\": \"B-OPP\", \"I\": \"I-OPP\"},\n",
        "    \"campaign\": {\"B\": \"B-CAMP\", \"I\": \"I-CAMP\"},\n",
        "    \"discount_code\": {\"B\": \"B-DISC\", \"I\": \"I-DISC\"},\n",
        "    \"custom1\": {\"B\": \"B-CUST1\", \"I\": \"I-CUST1\"},\n",
        "    \"custom2\": {\"B\": \"B-CUST2\", \"I\": \"I-CUST2\"},\n",
        "    \"username\": {\"B\": \"B-USERNAME\", \"I\": \"I-USERNAME\"},\n",
        "    \"address_line1\": {\"B\": \"B-ADDR1\", \"I\": \"I-ADDR1\"},\n",
        "    \"city\": {\"B\": \"B-CITY\", \"I\": \"I-CITY\"},\n",
        "    \"state\": {\"B\": \"B-STATE\", \"I\": \"I-STATE\"},\n",
        "    # Add other labels as needed\n",
        "}\n"
      ],
      "metadata": {
        "id": "qDwD9wBiQESi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define anomaly and normal templates\n",
        "anomaly_scenarios = [\n",
        "    \"Unauthorized login attempt detected for user {username} from IP {ip}.\",\n",
        "    \"Suspicious activity from IP address {ip} detected in {city}.\",\n",
        "    \"Multiple failed login attempts for {username} on device {device}.\",\n",
        "    \"Unexpected shutdown of the main {organization} server in {location}.\",\n",
        "    \"{username} reported a security breach in the {organization} affecting project {project}.\",\n",
        "    \"Intrusion detected in the {location} server room by IP {ip}.\",\n",
        "    \"Anomaly detected: unusual access patterns from IP {ip} targeting {project}.\",\n",
        "    \"Alert: {username} accessed restricted data without authorization from {city}.\",\n",
        "    \"System compromised: {organization} data integrity at risk due to {device}.\",\n",
        "    \"Abnormal behavior observed from user {username} at {city} accessing {url}.\",\n",
        "    \"Security alert: {username} attempted unauthorized access to {business_name} using {access_key}.\",\n",
        "    \"Hazmat spill reported at {address_line1}, {city}, {state} by {username}.\",\n",
        "    \"Emergency response initiated for {hazmat} incident at {location}.\",\n",
        "    \"Data leak detected involving {credit_card} from {device}.\",\n",
        "    \"{username} changed their password using device {device_id} from IP {ip}.\",\n",
        "    \"Multiple transactions flagged: {transaction} from {bank_account}.\",\n",
        "    \"Invalid access key {access_key} used by {username} from {ip}.\",\n",
        "    \"{username}'s social security number {social_security} was exposed during {event}.\",\n",
        "    \"License plate {license_plate} associated with unauthorized entry at {location}.\",\n",
        "    \"Customer {username} reported issue type {issue_type} with ticket {ticket_id}.\",\n",
        "    \"Anomaly in financial report: {money} discrepancy detected in {account}.\"\n",
        "]\n",
        "\n",
        "normal_scenarios = [\n",
        "    \"{username} accessed the secure server from IP {ip}.\",\n",
        "    \"The server located in {city} was rebooted at {date} {time}.\",\n",
        "    \"{username} updated their password successfully from device {device}.\",\n",
        "    \"System maintenance scheduled in {city} on {date} for {duration}.\",\n",
        "    \"Backup completed successfully for {project} using {device}.\",\n",
        "    \"{username} joined the {organization} team as a {role}.\",\n",
        "    \"{username} left the {organization}.\",\n",
        "    \"New project {project} has been initiated by {username}.\",\n",
        "    \"Meeting scheduled with {username} in {city} on {date} at {time}.\",\n",
        "    \"{username} submitted the quarterly report to {organization} via {url}.\",\n",
        "    \"{username} received an invoice {invoice} for project {project}.\",\n",
        "    \"Transaction {transaction} of {money} approved for account {account}.\",\n",
        "    \"Marketing campaign {campaign} launched with discount code {discount_code}.\",\n",
        "    \"Customer support ticket {ticket_id} assigned to {username} with priority {priority}.\",\n",
        "    \"{username} attended the {event} held at {location}.\",\n",
        "    \"Sales opportunity {opportunity} created by {username} in {campaign}.\",\n",
        "    \"{username} updated contact information including email {email} and phone {phone}.\",\n",
        "    \"Finance department reconciled bank account {bank_account} with transactions {transaction}.\",\n",
        "    \"{username} accessed CRM system using username {username} and device ID {device_id}.\",\n",
        "    \"Lead {lead} converted to opportunity {opportunity} by {username}.\",\n",
        "    \"HR team updated employee {username}'s role to {role}.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "yv85crfNQGO2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer with ModernBERT\n",
        "model_name = \"answerdotai/modernbert-large\"  # Replace with \"answerdotai/modernbert-base\" if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "0mcqORM6QH5D"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(anomaly=False, entity_lists=None, tokenizer=None, entity_label_map=None, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Generates a single synthetic data sample.\n",
        "\n",
        "    Args:\n",
        "        anomaly (bool): Whether to generate an anomaly sample.\n",
        "        entity_lists (dict): Dictionary containing all entity lists.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing 'text', 'ner_labels', 'anomaly_label', and 'entities'.\n",
        "    \"\"\"\n",
        "    if entity_lists is None:\n",
        "        entity_lists = {}\n",
        "\n",
        "    if entity_label_map is None:\n",
        "        raise ValueError(\"entity_label_map must be provided\")\n",
        "\n",
        "    if tokenizer is None:\n",
        "        raise ValueError(\"tokenizer must be provided\")\n",
        "\n",
        "    if anomaly:\n",
        "        template = random.choice(anomaly_scenarios)\n",
        "        anomaly_label = 1\n",
        "    else:\n",
        "        template = random.choice(normal_scenarios)\n",
        "        anomaly_label = 0\n",
        "\n",
        "    # Find all placeholders in the template\n",
        "    placeholders = re.findall(r\"\\{(.*?)\\}\", template)\n",
        "    unique_placeholders = list(set(placeholders))\n",
        "\n",
        "    # Initialize entity selections\n",
        "    selected_entities = {}\n",
        "\n",
        "    # Select entities for each placeholder type\n",
        "    for placeholder in unique_placeholders:\n",
        "        if placeholder in entity_lists and len(entity_lists[placeholder]) > 0:\n",
        "            selected_entities[placeholder] = random.choice(entity_lists[placeholder])\n",
        "        else:\n",
        "            selected_entities[placeholder] = \"Unknown\"  # Fallback for undefined placeholders\n",
        "\n",
        "    # Replace placeholders with selected entities\n",
        "    filled_text = template\n",
        "    for placeholder, entity in selected_entities.items():\n",
        "        filled_text = filled_text.replace(f\"{{{placeholder}}}\", entity)\n",
        "\n",
        "    # Prepare entities list with character offsets\n",
        "    entities = []\n",
        "    for placeholder, entity in selected_entities.items():\n",
        "        # Find all occurrences of the entity in text to handle multiple instances\n",
        "        start_indices = [m.start() for m in re.finditer(re.escape(entity), filled_text)]\n",
        "        for start_char in start_indices:\n",
        "            end_char = start_char + len(entity)\n",
        "            entities.append({\n",
        "                \"text\": entity,\n",
        "                \"type\": placeholder,\n",
        "                \"start_char\": start_char,\n",
        "                \"end_char\": end_char\n",
        "            })\n",
        "\n",
        "    # Tokenize and assign labels\n",
        "    ner_labels = tokenize_and_align_labels(filled_text, entities, tokenizer, entity_label_map, O_label)\n",
        "\n",
        "    return {\n",
        "        \"text\": filled_text,\n",
        "        \"ner_labels\": ner_labels,\n",
        "        \"anomaly_label\": anomaly_label,\n",
        "        \"entities\": entities  # Include entities for validation\n",
        "    }\n",
        "\n",
        "def tokenize_and_align_labels(text, entities, tokenizer, entity_label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Tokenizes the text and aligns the NER labels with the tokenized output using character offsets.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        entities (List[Dict]): A list of entities with 'text', 'type', 'start_char', and 'end_char'.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of BIO labels aligned with the tokenized text.\n",
        "    \"\"\"\n",
        "    # Initialize labels as 'O'\n",
        "    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "    offsets = encoding['offset_mapping']\n",
        "    labels = [O_label] * len(encoding['input_ids'])\n",
        "\n",
        "    # Process each entity\n",
        "    for entity in entities:\n",
        "        entity_type = entity[\"type\"].lower()\n",
        "        start_char = entity[\"start_char\"]\n",
        "        end_char = entity[\"end_char\"]\n",
        "        # Assign labels to tokens\n",
        "        for idx, (token_start, token_end) in enumerate(offsets):\n",
        "            if token_start >= end_char:\n",
        "                break\n",
        "            if token_end <= start_char:\n",
        "                continue\n",
        "            if token_start >= start_char and token_end <= end_char:\n",
        "                if token_start == start_char:\n",
        "                    label_key = entity_label_map.get(entity_type, {}).get(\"B\", O_label)\n",
        "                    labels[idx] = label_key\n",
        "                else:\n",
        "                    label_key = entity_label_map.get(entity_type, {}).get(\"I\", O_label)\n",
        "                    labels[idx] = label_key\n",
        "\n",
        "    return labels\n",
        "\n",
        "def generate_dataset(num_samples=5000, anomaly_ratio=0.3, seed=None, entity_lists=None, tokenizer=None, entity_label_map=None, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Total number of samples to generate.\n",
        "        anomaly_ratio (float): Proportion of samples that are anomalies.\n",
        "        seed (int, optional): Random seed for reproducibility.\n",
        "        entity_lists (dict): Dictionary containing all entity lists.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of synthetic data samples.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    dataset = []\n",
        "    for _ in tqdm(range(num_samples), desc=\"Generating Synthetic Data\"):\n",
        "        is_anomaly = random.random() < anomaly_ratio\n",
        "        sample = generate_sample(\n",
        "            anomaly=is_anomaly,\n",
        "            entity_lists=entity_lists,\n",
        "            tokenizer=tokenizer,\n",
        "            entity_label_map=entity_label_map,\n",
        "            O_label=O_label\n",
        "        )\n",
        "        dataset.append(sample)\n",
        "\n",
        "    # Shuffle the dataset to mix anomaly and normal samples\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "EZbWDzFJMrkh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all entities for easy access\n",
        "entity_lists = {\n",
        "    \"person\": persons,\n",
        "    \"alias\": aliases,\n",
        "    \"title\": titles,\n",
        "    \"role\": roles,\n",
        "    \"organization\": organizations,\n",
        "    \"business_name\": business_names,\n",
        "    \"business_id\": business_ids,\n",
        "    \"location\": locations,\n",
        "    \"ip\": ips,\n",
        "    \"mac_address\": mac_addresses,\n",
        "    \"project\": projects,\n",
        "    \"date\": dates,\n",
        "    \"time\": times,\n",
        "    \"duration\": durations,\n",
        "    \"event\": events,\n",
        "    \"email\": emails,\n",
        "    \"phone\": phones,\n",
        "    \"url\": urls,\n",
        "    \"device\": devices,\n",
        "    \"device_id\": device_ids,\n",
        "    \"password\": passwords,\n",
        "    \"access_key\": access_keys,\n",
        "    \"social_security\": social_security_numbers,\n",
        "    \"credit_card\": credit_cards,\n",
        "    \"bank_account\": bank_accounts,\n",
        "    \"license_plate\": license_plates,\n",
        "    \"hazmat\": hazmats,\n",
        "    \"money\": money,\n",
        "    \"currency\": currencies,\n",
        "    \"invoice\": invoices,\n",
        "    \"transaction\": transactions,\n",
        "    \"account\": accounts,\n",
        "    \"ticket_id\": ticket_ids,\n",
        "    \"issue_type\": issue_types,\n",
        "    \"priority\": priorities,\n",
        "    \"resolution_status\": resolution_statuses,\n",
        "    \"lead\": leads,\n",
        "    \"opportunity\": opportunities,\n",
        "    \"campaign\": campaigns,\n",
        "    \"discount_code\": discount_codes,\n",
        "    \"custom1\": custom1,\n",
        "    \"custom2\": custom2,\n",
        "    \"username\": username,\n",
        "    \"address_line1\": address_line1,\n",
        "    \"city\": city,\n",
        "    \"state\": state\n",
        "}\n",
        "\n",
        "# Generate synthetic training data\n",
        "synthetic_training_data = generate_dataset(\n",
        "    num_samples=5000,\n",
        "    anomaly_ratio=0.3,\n",
        "    seed=42,\n",
        "    entity_lists=entity_lists,\n",
        "    tokenizer=tokenizer,\n",
        "    entity_label_map=entity_label_map,\n",
        "    O_label=O_label\n",
        ")\n",
        "\n",
        "# Display first 5 samples for verification\n",
        "for i, sample in enumerate(synthetic_training_data[:5], 1):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"Text: {sample['text']}\")\n",
        "    print(f\"NER Labels: {sample['ner_labels']}\")\n",
        "    print(f\"Anomaly Label: {sample['anomaly_label']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udwj6iw5QNwT",
        "outputId": "d95bf7dd-f8d1-4c7f-bd60-91f861f61c18"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Synthetic Data: 100%|██████████| 5000/5000 [00:01<00:00, 4927.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Text: guest901 attended the Annual Meeting held at New York.\n",
            "NER Labels: ['B-USERNAME', 'I-USERNAME', 'I-USERNAME', 'O', 'O', 'O', 'I-EVENT', 'O', 'O', 'O', 'I-LOC', 'O']\n",
            "Anomaly Label: 0\n",
            "\n",
            "Sample 2:\n",
            "Text: Emergency response initiated for Hazmat Substance B incident at Denver.\n",
            "NER Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'I-HAZMAT', 'I-HAZMAT', 'I-HAZMAT', 'I-HAZMAT', 'O', 'O', 'O', 'O']\n",
            "Anomaly Label: 1\n",
            "\n",
            "Sample 3:\n",
            "Text: Anomaly detected: unusual access patterns from IP 172.16.0.3 targeting Apollo.\n",
            "NER Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-IP', 'I-IP', 'I-IP', 'I-IP', 'I-IP', 'I-IP', 'O', 'O', 'O']\n",
            "Anomaly Label: 1\n",
            "\n",
            "Sample 4:\n",
            "Text: Anomaly in financial report: $500 discrepancy detected in ACCT3009.\n",
            "NER Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MONEY', 'O', 'O', 'O', 'O', 'I-ACCT', 'I-ACCT', 'I-ACCT', 'O']\n",
            "Anomaly Label: 1\n",
            "\n",
            "Sample 5:\n",
            "Text: Marketing campaign Camp3006 launched with discount code DISC80.\n",
            "NER Labels: ['O', 'O', 'O', 'O', 'I-CAMP', 'I-CAMP', 'O', 'O', 'O', 'O', 'O', 'I-DISC', 'I-DISC', 'O']\n",
            "Anomaly Label: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label_to_id mapping\n",
        "def create_label_to_id_map(label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Creates a mapping from label strings to unique integer IDs.\n",
        "\n",
        "    Args:\n",
        "        label_map (Dict[str, Dict[str, str]]): Updated entity label map.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Mapping from label strings to unique IDs.\n",
        "    \"\"\"\n",
        "    unique_labels = set()\n",
        "    for entity, sub_map in label_map.items():\n",
        "        if isinstance(sub_map, dict):\n",
        "            unique_labels.update(sub_map.values())\n",
        "        else:\n",
        "            unique_labels.add(sub_map)  # Handle 'O' if present\n",
        "\n",
        "    unique_labels.add(O_label)  # Ensure 'O' is included\n",
        "\n",
        "    sorted_labels = sorted(unique_labels)  # Sorting for consistency\n",
        "    label_to_id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
        "    return label_to_id\n",
        "\n",
        "# Create the label_to_id mapping\n",
        "label_to_id = create_label_to_id_map(entity_label_map, O_label=O_label)\n",
        "print(\"Label to ID mapping:\", label_to_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MODyjrx6Mv9h",
        "outputId": "26fd099a-3da1-4bd0-d079-c3d0afc7bad8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label to ID mapping: {'B-ACCT': 0, 'B-ADDR1': 1, 'B-ALIAS': 2, 'B-BANK': 3, 'B-BUS': 4, 'B-BUSID': 5, 'B-CAMP': 6, 'B-CC': 7, 'B-CITY': 8, 'B-CUR': 9, 'B-CUST1': 10, 'B-CUST2': 11, 'B-DATE': 12, 'B-DEV': 13, 'B-DEV_ID': 14, 'B-DISC': 15, 'B-DUR': 16, 'B-EMAIL': 17, 'B-EVENT': 18, 'B-HAZMAT': 19, 'B-INVOICE': 20, 'B-IP': 21, 'B-ISSUE': 22, 'B-KEY': 23, 'B-LEAD': 24, 'B-LOC': 25, 'B-MAC': 26, 'B-MONEY': 27, 'B-OPP': 28, 'B-ORG': 29, 'B-PASS': 30, 'B-PER': 31, 'B-PHONE': 32, 'B-PLATE': 33, 'B-PRIORITY': 34, 'B-PROJ': 35, 'B-ROLE': 36, 'B-SSN': 37, 'B-STATE': 38, 'B-STATUS': 39, 'B-TICKET': 40, 'B-TIME': 41, 'B-TITLE': 42, 'B-TRANS': 43, 'B-URL': 44, 'B-USERNAME': 45, 'I-ACCT': 46, 'I-ADDR1': 47, 'I-ALIAS': 48, 'I-BANK': 49, 'I-BUS': 50, 'I-BUSID': 51, 'I-CAMP': 52, 'I-CC': 53, 'I-CITY': 54, 'I-CUR': 55, 'I-CUST1': 56, 'I-CUST2': 57, 'I-DATE': 58, 'I-DEV': 59, 'I-DEV_ID': 60, 'I-DISC': 61, 'I-DUR': 62, 'I-EMAIL': 63, 'I-EVENT': 64, 'I-HAZMAT': 65, 'I-INVOICE': 66, 'I-IP': 67, 'I-ISSUE': 68, 'I-KEY': 69, 'I-LEAD': 70, 'I-LOC': 71, 'I-MAC': 72, 'I-MONEY': 73, 'I-OPP': 74, 'I-ORG': 75, 'I-PASS': 76, 'I-PER': 77, 'I-PHONE': 78, 'I-PLATE': 79, 'I-PRIORITY': 80, 'I-PROJ': 81, 'I-ROLE': 82, 'I-SSN': 83, 'I-STATE': 84, 'I-STATUS': 85, 'I-TICKET': 86, 'I-TIME': 87, 'I-TITLE': 88, 'I-TRANS': 89, 'I-URL': 90, 'I-USERNAME': 91, 'O': 92}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label_to_id mapping\n",
        "def create_label_to_id_map(label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Creates a mapping from label strings to unique integer IDs.\n",
        "\n",
        "    Args:\n",
        "        label_map (Dict[str, Dict[str, str]]): Updated entity label map.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Mapping from label strings to unique IDs.\n",
        "    \"\"\"\n",
        "    unique_labels = set()\n",
        "    for entity, sub_map in label_map.items():\n",
        "        if isinstance(sub_map, dict):\n",
        "            unique_labels.update(sub_map.values())\n",
        "        else:\n",
        "            unique_labels.add(sub_map)  # Handle 'O' if present\n",
        "\n",
        "    unique_labels.add(O_label)  # Ensure 'O' is included\n",
        "\n",
        "    sorted_labels = sorted(unique_labels)  # Sorting for consistency\n",
        "    label_to_id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
        "    return label_to_id\n",
        "\n",
        "# Create the label_to_id mapping\n",
        "label_to_id = create_label_to_id_map(entity_label_map, O_label=O_label)\n",
        "print(\"Label to ID mapping:\", label_to_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NZqgdQsNgBL",
        "outputId": "70e9d1f4-136b-4270-98de-6731793d285e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label to ID mapping: {'B-ACCT': 0, 'B-ADDR1': 1, 'B-ALIAS': 2, 'B-BANK': 3, 'B-BUS': 4, 'B-BUSID': 5, 'B-CAMP': 6, 'B-CC': 7, 'B-CITY': 8, 'B-CUR': 9, 'B-CUST1': 10, 'B-CUST2': 11, 'B-DATE': 12, 'B-DEV': 13, 'B-DEV_ID': 14, 'B-DISC': 15, 'B-DUR': 16, 'B-EMAIL': 17, 'B-EVENT': 18, 'B-HAZMAT': 19, 'B-INVOICE': 20, 'B-IP': 21, 'B-ISSUE': 22, 'B-KEY': 23, 'B-LEAD': 24, 'B-LOC': 25, 'B-MAC': 26, 'B-MONEY': 27, 'B-OPP': 28, 'B-ORG': 29, 'B-PASS': 30, 'B-PER': 31, 'B-PHONE': 32, 'B-PLATE': 33, 'B-PRIORITY': 34, 'B-PROJ': 35, 'B-ROLE': 36, 'B-SSN': 37, 'B-STATE': 38, 'B-STATUS': 39, 'B-TICKET': 40, 'B-TIME': 41, 'B-TITLE': 42, 'B-TRANS': 43, 'B-URL': 44, 'B-USERNAME': 45, 'I-ACCT': 46, 'I-ADDR1': 47, 'I-ALIAS': 48, 'I-BANK': 49, 'I-BUS': 50, 'I-BUSID': 51, 'I-CAMP': 52, 'I-CC': 53, 'I-CITY': 54, 'I-CUR': 55, 'I-CUST1': 56, 'I-CUST2': 57, 'I-DATE': 58, 'I-DEV': 59, 'I-DEV_ID': 60, 'I-DISC': 61, 'I-DUR': 62, 'I-EMAIL': 63, 'I-EVENT': 64, 'I-HAZMAT': 65, 'I-INVOICE': 66, 'I-IP': 67, 'I-ISSUE': 68, 'I-KEY': 69, 'I-LEAD': 70, 'I-LOC': 71, 'I-MAC': 72, 'I-MONEY': 73, 'I-OPP': 74, 'I-ORG': 75, 'I-PASS': 76, 'I-PER': 77, 'I-PHONE': 78, 'I-PLATE': 79, 'I-PRIORITY': 80, 'I-PROJ': 81, 'I-ROLE': 82, 'I-SSN': 83, 'I-STATE': 84, 'I-STATUS': 85, 'I-TICKET': 86, 'I-TIME': 87, 'I-TITLE': 88, 'I-TRANS': 89, 'I-URL': 90, 'I-USERNAME': 91, 'O': 92}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JointNERAnomalyDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, label_to_id, max_length=128):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data (List[Dict]): The synthetic dataset.\n",
        "            tokenizer: The tokenizer instance.\n",
        "            label_to_id (Dict[str, int]): Mapping from label strings to IDs.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_to_id = label_to_id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        ner_labels = item['ner_labels']\n",
        "        anomaly_label = item['anomaly_label']\n",
        "        entities = item.get('entities', [])  # Retrieve entities if available\n",
        "\n",
        "        # Tokenize the input text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        offsets = encoding['offset_mapping'].squeeze().tolist()\n",
        "\n",
        "        # Initialize labels as 'O'\n",
        "        labels = [O_label] * len(input_ids)\n",
        "\n",
        "        # Assign labels based on entities\n",
        "        for entity in entities:\n",
        "            entity_type = entity[\"type\"].lower()  # Ensure lowercase for consistency\n",
        "            start_char = entity[\"start_char\"]\n",
        "            end_char = entity[\"end_char\"]\n",
        "            for idx_token, (token_start, token_end) in enumerate(offsets):\n",
        "                if token_start >= end_char:\n",
        "                    break\n",
        "                if token_end <= start_char:\n",
        "                    continue\n",
        "                if token_start >= start_char and token_end <= end_char:\n",
        "                    if token_start == start_char:\n",
        "                        labels[idx_token] = entity_label_map[entity_type][\"B\"]\n",
        "                    else:\n",
        "                        labels[idx_token] = entity_label_map[entity_type][\"I\"]\n",
        "\n",
        "        # Convert labels to IDs\n",
        "        ner_label_ids = [self.label_to_id.get(label, self.label_to_id[O_label]) for label in labels]\n",
        "\n",
        "        # Handle padding labels\n",
        "        if len(ner_label_ids) < self.max_length:\n",
        "            ner_label_ids += [self.label_to_id[O_label]] * (self.max_length - len(ner_label_ids))\n",
        "        elif len(ner_label_ids) > self.max_length:\n",
        "            ner_label_ids = ner_label_ids[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'ner_labels': torch.tensor(ner_label_ids, dtype=torch.long),\n",
        "            'anomaly_labels': torch.tensor(anomaly_label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "CpP8lqKFMyHX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataset\n",
        "combined_dataset = JointNERAnomalyDataset(\n",
        "    data=synthetic_training_data,\n",
        "    tokenizer=tokenizer,\n",
        "    label_to_id=label_to_id,\n",
        "    max_length=128  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Split into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(combined_dataset))\n",
        "val_size = len(combined_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of5HHdrmQbn5",
        "outputId": "ac98ea34-22d1-41b8-965e-93511390d8a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 4000\n",
            "Validation samples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "GD3t1lvYQeFg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the DataLoader to verify no KeyErrors occur\n",
        "try:\n",
        "    for batch in train_loader:\n",
        "        print(\"Batch successfully loaded:\")\n",
        "        print({\n",
        "            'input_ids': batch['input_ids'].shape,\n",
        "            'attention_mask': batch['attention_mask'].shape,\n",
        "            'ner_labels': batch['ner_labels'].shape,\n",
        "            'anomaly_labels': batch['anomaly_labels'].shape\n",
        "        })\n",
        "        break  # Only verify the first batch\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError encountered: {e}. Please ensure all entity types are mapped.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnQc6Y4QQimT",
        "outputId": "dfefb931-fdab-490c-ad7a-2e7ecef81a24"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch successfully loaded:\n",
            "{'input_ids': torch.Size([16, 128]), 'attention_mask': torch.Size([16, 128]), 'ner_labels': torch.Size([16, 128]), 'anomaly_labels': torch.Size([16])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class JointNERAnomalyModel(nn.Module):\n",
        "    def __init__(self, base_model, num_ner_labels, num_anomaly_labels):\n",
        "        super(JointNERAnomalyModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.hidden_size = base_model.config.hidden_size\n",
        "\n",
        "        # NER head\n",
        "        self.ner_classifier = nn.Linear(self.hidden_size, num_ner_labels)\n",
        "\n",
        "        # Anomaly Detection head\n",
        "        self.anomaly_classifier = nn.Linear(self.hidden_size, num_anomaly_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state  # For NER\n",
        "\n",
        "        # Use the CLS token's hidden state for anomaly classification\n",
        "        cls_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        ner_logits = self.ner_classifier(sequence_output)\n",
        "        anomaly_logits = self.anomaly_classifier(cls_output)\n",
        "\n",
        "        return ner_logits, anomaly_logits\n",
        "\n"
      ],
      "metadata": {
        "id": "9qPjcEjzQlCG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ModernBERT model\n",
        "base_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define number of labels\n",
        "num_ner_labels = len(label_to_id)  # Total unique NER labels\n",
        "num_anomaly_labels = 2  # Binary classification: Normal or Anomaly\n",
        "\n",
        "# Initialize the joint model\n",
        "model = JointNERAnomalyModel(base_model, num_ner_labels, num_anomaly_labels)\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Define loss functions\n",
        "ner_loss_fn = nn.CrossEntropyLoss(ignore_index=label_to_id[O_label])  # Ignore 'O' label in loss\n",
        "anomaly_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "df169e2a7af04432a088c67beb8c3782",
            "4b4053e0b86844669d62a568a175f3d4",
            "18ad30c72480430fa1965ed8b5f599b5",
            "155bae3009e444a282085f8598991dcb",
            "71911bb906fd4c9690f14cbe2ce80be9",
            "6d85c19c0a014fe2bcacfdb629545bf4",
            "0cf11862a7864af386b4de48bedc0ebb",
            "a71df83a44914ae091b8c5049a0ef747",
            "08e5f36f4dad4f658dfcfa9d7f9ea17e",
            "0a8e045cef124b8c844fb93e6d5bad46",
            "5d202ccf4c084ab2b05092fdeaf55417"
          ]
        },
        "id": "Gbvsab89QnLA",
        "outputId": "e18af3a5-9cc8-4e6e-d5ff-8dc9d0b7b6f6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df169e2a7af04432a088c67beb8c3782"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3  # Adjust based on your requirements and Colab's runtime limits\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "8CScOIZ7QrIG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        ner_labels = batch['ner_labels'].to(device)\n",
        "        anomaly_labels = batch['anomaly_labels'].to(device)\n",
        "\n",
        "        ner_logits, anomaly_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute NER loss\n",
        "        ner_loss = ner_loss_fn(ner_logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Compute Anomaly Detection loss\n",
        "        anomaly_loss = anomaly_loss_fn(anomaly_logits, anomaly_labels)\n",
        "\n",
        "        # Total loss\n",
        "        loss = ner_loss + anomaly_loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQLwLZkGQtFo",
        "outputId": "f25ec524-f7af-49bf-cc30-6c6697986da4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 250/250 [05:48<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Average Loss: 0.34516885855142027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 250/250 [05:45<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Average Loss: 0.0005255920746130868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 250/250 [05:45<00:00,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Average Loss: 0.00032167482742806897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, label_to_id, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        dataloader: DataLoader for evaluation.\n",
        "        label_to_id: Dictionary mapping labels to IDs.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ner_preds = []\n",
        "    ner_true = []\n",
        "    anomaly_preds = []\n",
        "    anomaly_true = []\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            ner_labels = batch['ner_labels'].to(device)\n",
        "            anomaly_labels = batch['anomaly_labels'].to(device)\n",
        "\n",
        "            ner_logits, anomaly_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # NER Predictions\n",
        "            ner_pred = torch.argmax(ner_logits, dim=-1).cpu().numpy()\n",
        "            ner_true_labels = ner_labels.cpu().numpy()\n",
        "            ner_preds.extend(ner_pred.flatten())\n",
        "            ner_true.extend(ner_true_labels.flatten())\n",
        "\n",
        "            # Anomaly Predictions\n",
        "            anomaly_pred = torch.argmax(anomaly_logits, dim=-1).cpu().numpy()\n",
        "            anomaly_true_labels = anomaly_labels.cpu().numpy()\n",
        "            anomaly_preds.extend(anomaly_pred)\n",
        "            anomaly_true.extend(anomaly_true_labels)\n",
        "\n",
        "    # Remove padding tokens ('O' label)\n",
        "    o_label_id = label_to_id.get(O_label, -1)\n",
        "    valid_indices = [i for i, label in enumerate(ner_true) if label != o_label_id]\n",
        "    filtered_ner_preds = [ner_preds[i] for i in valid_indices]\n",
        "    filtered_ner_true = [ner_true[i] for i in valid_indices]\n",
        "\n",
        "    # Map label IDs back to labels\n",
        "    filtered_ner_preds_labels = [id_to_label.get(id, O_label) for id in filtered_ner_preds]\n",
        "    filtered_ner_true_labels = [id_to_label.get(id, O_label) for id in filtered_ner_true]\n",
        "\n",
        "    # NER Classification Report\n",
        "    print(\"NER Classification Report:\")\n",
        "    print(classification_report(filtered_ner_true_labels, filtered_ner_preds_labels, digits=4))\n",
        "\n",
        "    # Anomaly Detection Classification Report\n",
        "    print(\"Anomaly Detection Classification Report:\")\n",
        "    print(classification_report(anomaly_true, anomaly_preds, digits=4))\n",
        "\n",
        "    # Plot Confusion Matrix for Anomaly Detection\n",
        "    plt.figure(figsize=(6,6))\n",
        "    cm = confusion_matrix(anomaly_true, anomaly_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Anomaly Detection Confusion Matrix')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZtDJZO_tQ0AV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "model_dir = \"/content/drive/MyDrive/models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the model to Google Drive\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"joint_ner_anomaly_model_lg.pth\"))\n",
        "print(f\"Model saved to {model_dir}/joint_ner_anomaly_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT0G35p0SIwv",
        "outputId": "be61538d-c187-42c3-92da-8b40504190ee"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model saved to /content/drive/MyDrive/models/joint_ner_anomaly_model.pth\n"
          ]
        }
      ]
    }
  ]
}