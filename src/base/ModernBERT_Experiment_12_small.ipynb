{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMX+hAfcRAW5UYQmoIkDyH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CogNetSys/ModernBERT/blob/main/ModernBERT_Experiment_12_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-M7yO3VKDKT"
      },
      "outputs": [],
      "source": [
        "# Install Hugging Face Transformers and other dependencies\n",
        "!pip install transformers==4.48.0\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install seaborn\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate installation\n",
        "!python3 -m spacy validate"
      ],
      "metadata": {
        "id": "5EF2fUCXKWvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "PoKUpGExKPAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define comprehensive entity lists\n",
        "persons = [\n",
        "    \"John Doe\", \"Alice Smith\", \"Maria Garcia\", \"Bob Johnson\", \"Charlie Lee\",\n",
        "    \"David Brown\", \"Emma Wilson\", \"Frank Moore\", \"Grace Taylor\", \"Henry Anderson\"\n",
        "]\n",
        "aliases = [\n",
        "    \"Johnny\", \"Ally\", \"Mia\", \"Bobby\", \"Chuck\",\n",
        "    \"Dave\", \"Em\", \"Frankie\", \"Gracie\", \"Hank\"\n",
        "]\n",
        "titles = [\n",
        "    \"Dr.\", \"Prof.\", \"Mr.\", \"Ms.\", \"Mrs.\",\n",
        "    \"CEO\", \"CTO\", \"Manager\", \"Director\", \"Lead\"\n",
        "]\n",
        "roles = [\n",
        "    \"Software Engineer\", \"Data Scientist\", \"Product Manager\", \"Sales Executive\", \"HR Specialist\",\n",
        "    \"Marketing Coordinator\", \"Financial Analyst\", \"Customer Support Representative\", \"DevOps Engineer\", \"UX Designer\"\n",
        "]\n",
        "organizations = [\n",
        "    \"Acme Corp\", \"Global Tech\", \"Finance Department\", \"HR Team\", \"IT Services\",\n",
        "    \"Research Division\", \"Marketing Department\", \"Sales Team\", \"Operations Unit\", \"Customer Support\"\n",
        "]\n",
        "business_names = organizations.copy()  # Assuming business names align with organizations\n",
        "business_ids = [\n",
        "    \"BUS123456\", \"BUS234567\", \"BUS345678\", \"BUS456789\", \"BUS567890\",\n",
        "    \"BUS678901\", \"BUS789012\", \"BUS890123\", \"BUS901234\", \"BUS012345\"\n",
        "]\n",
        "locations = [\n",
        "    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\",\n",
        "    \"San Francisco\", \"Boston\", \"Seattle\", \"Denver\", \"Miami\"\n",
        "]\n",
        "ips = [\n",
        "    \"192.168.1.1\", \"10.0.0.5\", \"172.16.0.3\", \"192.168.0.100\", \"10.0.1.25\",\n",
        "    \"192.168.1.255\", \"10.0.0.8\", \"172.16.0.45\", \"192.168.0.55\", \"10.0.1.99\"\n",
        "]\n",
        "mac_addresses = [\n",
        "    \"00:1A:2B:3C:4D:5E\", \"11:22:33:44:55:66\", \"AA:BB:CC:DD:EE:FF\",\n",
        "    \"12:34:56:78:9A:BC\", \"DE:F0:12:34:56:78\",\n",
        "    \"98:76:54:32:10:FE\", \"AB:CD:EF:12:34:56\", \"12:AB:34:CD:56:EF\",\n",
        "    \"FE:DC:BA:98:76:54\", \"65:43:21:09:87:65\"\n",
        "]\n",
        "projects = [\n",
        "    \"Project Phoenix\", \"Apollo\", \"Zeus\", \"Hermes\", \"Athena\",\n",
        "    \"Project Titan\", \"Orion\", \"Elysium\", \"Nebula\", \"Vortex\"\n",
        "]\n",
        "dates = [\n",
        "    \"Monday\", \"Tuesday\", \"Wednesday\", \"Saturday\", \"Sunday\",\n",
        "    \"April 5th\", \"July 20th\", \"September 15th\", \"December 1st\", \"January 10th\"\n",
        "]\n",
        "times = [\n",
        "    \"10:00 AM\", \"2:30 PM\", \"5:45 PM\", \"9:15 AM\", \"1:00 PM\",\n",
        "    \"3:20 PM\", \"4:50 PM\", \"11:30 AM\", \"6:00 PM\", \"8:15 PM\"\n",
        "]\n",
        "durations = [\n",
        "    \"2 hours\", \"30 minutes\", \"45 minutes\", \"1 hour\", \"3 hours\",\n",
        "    \"15 minutes\", \"1.5 hours\", \"4 hours\", \"25 minutes\", \"50 minutes\"\n",
        "]\n",
        "events = [\n",
        "    \"Annual Meeting\", \"Quarterly Review\", \"Product Launch\", \"Team Building Retreat\", \"Client Presentation\",\n",
        "    \"Security Audit\", \"System Upgrade\", \"Sales Conference\", \"Marketing Workshop\", \"HR Training\"\n",
        "]\n",
        "emails = [\n",
        "    \"john.doe@example.com\", \"alice.smith@globaltech.com\", \"maria.garcia@acmecorp.com\",\n",
        "    \"bob.johnson@finance.dept.com\", \"charlie.lee@itservices.com\"\n",
        "]\n",
        "phones = [\n",
        "    \"+1-202-555-0156\", \"+1-303-555-0198\", \"+1-404-555-0133\", \"+1-505-555-0177\", \"+1-606-555-0111\",\n",
        "    \"+1-707-555-0144\", \"+1-808-555-0188\", \"+1-909-555-0122\", \"+1-101-555-0166\", \"+1-212-555-0100\"\n",
        "]\n",
        "urls = [\n",
        "    \"https://acme.com/login\", \"https://globaltech.com/dashboard\", \"https://finance.dept.com/reports\",\n",
        "    \"https://hrteam.com/profile\", \"https://itservices.com/support\",\n",
        "    \"https://research.division.com/data\", \"https://marketing.dept.com/campaigns\", \"https://sales.team.com/leads\",\n",
        "    \"https://operations.unit.com/status\", \"https://customersupport.com/help\"\n",
        "]\n",
        "devices = [\n",
        "    \"Laptop-01\", \"Server-12\", \"Router-5\", \"Firewall-3\", \"Workstation-7\",\n",
        "    \"Tablet-4\", \"Smartphone-9\", \"Printer-2\", \"Scanner-6\", \"NAS-8\"\n",
        "]\n",
        "device_ids = devices.copy()  # Assuming device IDs align with devices\n",
        "passwords = [\n",
        "    \"P@ssw0rd!\", \"Secure#123\", \"Admin@2024\", \"User*Pass1\", \"Qwerty!234\",\n",
        "    \"Welcome#1\", \"Passw0rd$\", \"Login*123\", \"MyPass#456\", \"Access@789\"\n",
        "]\n",
        "access_keys = [\n",
        "    \"AK12345XYZ\", \"AK67890ABC\", \"AK54321DEF\", \"AK09876GHI\", \"AK11223JKL\",\n",
        "    \"AK44556MNO\", \"AK77889PQR\", \"AK99000STU\", \"AK13579VWX\", \"AK24680YZA\"\n",
        "]\n",
        "social_security_numbers = [\n",
        "    \"123-45-6789\", \"987-65-4321\", \"555-55-5555\", \"111-22-3333\", \"444-55-6666\",\n",
        "    \"777-88-9999\", \"222-33-4444\", \"333-44-5555\", \"666-77-8888\", \"999-00-1111\"\n",
        "]\n",
        "credit_cards = [\n",
        "    \"4111-1111-1111-1111\", \"5500-0000-0000-0004\", \"3400-0000-0000-009\", \"3000-0000-0000-04\",\n",
        "    \"6011-0000-0000-0004\", \"2014-0000-0000-009\", \"3088-0000-0000-0009\", \"3600-0000-0000-0008\",\n",
        "    \"3800-0000-0000-0028\", \"6304-0000-0000-0003\"\n",
        "]\n",
        "bank_accounts = [\n",
        "    \"BA123456789\", \"BA987654321\", \"BA555555555\", \"BA111222333\", \"BA444555666\",\n",
        "    \"BA777888999\", \"BA000111222\", \"BA333444555\", \"BA666777888\", \"BA999000111\"\n",
        "]\n",
        "license_plates = [\n",
        "    \"ABC-1234\", \"XYZ-5678\", \"LMN-9012\", \"DEF-3456\", \"GHI-7890\",\n",
        "    \"JKL-2345\", \"MNO-6789\", \"PQR-0123\", \"STU-4567\", \"VWX-8901\"\n",
        "]\n",
        "hazmats = [\n",
        "    \"Hazmat Material A\", \"Hazmat Substance B\", \"Hazmat Agent C\", \"Hazmat Compound D\", \"Hazmat Material E\",\n",
        "    \"Hazmat Substance F\", \"Hazmat Agent G\", \"Hazmat Compound H\", \"Hazmat Material I\", \"Hazmat Substance J\"\n",
        "]\n",
        "money = [\n",
        "    \"$1000\", \"$2500\", \"$500\", \"$750\", \"$1200\",\n",
        "    \"$3000\", \"$450\", \"$600\", \"$800\", \"$950\"\n",
        "]\n",
        "currencies = [\n",
        "    \"USD\", \"EUR\", \"GBP\", \"JPY\", \"AUD\",\n",
        "    \"CAD\", \"CHF\", \"CNY\", \"SEK\", \"NZD\"\n",
        "]\n",
        "invoices = [\n",
        "    \"INV1001\", \"INV1002\", \"INV1003\", \"INV1004\", \"INV1005\",\n",
        "    \"INV1006\", \"INV1007\", \"INV1008\", \"INV1009\", \"INV1010\"\n",
        "]\n",
        "transactions = [\n",
        "    \"TXN5001\", \"TXN5002\", \"TXN5003\", \"TXN5004\", \"TXN5005\",\n",
        "    \"TXN5006\", \"TXN5007\", \"TXN5008\", \"TXN5009\", \"TXN5010\"\n",
        "]\n",
        "accounts = [\n",
        "    \"ACCT3001\", \"ACCT3002\", \"ACCT3003\", \"ACCT3004\", \"ACCT3005\",\n",
        "    \"ACCT3006\", \"ACCT3007\", \"ACCT3008\", \"ACCT3009\", \"ACCT3010\"\n",
        "]\n",
        "ticket_ids = [\n",
        "    \"TICKET1001\", \"TICKET1002\", \"TICKET1003\", \"TICKET1004\", \"TICKET1005\",\n",
        "    \"TICKET1006\", \"TICKET1007\", \"TICKET1008\", \"TICKET1009\", \"TICKET1010\"\n",
        "]\n",
        "issue_types = [\n",
        "    \"Login Issue\", \"Password Reset\", \"Account Lockout\", \"Data Breach\", \"System Downtime\",\n",
        "    \"Payment Failure\", \"Feature Request\", \"Bug Report\", \"Access Denied\", \"Performance Lag\"\n",
        "]\n",
        "priorities = [\n",
        "    \"Low\", \"Medium\", \"High\", \"Critical\", \"Urgent\",\n",
        "    \"Low\", \"Medium\", \"High\", \"Critical\", \"Urgent\"\n",
        "]\n",
        "resolution_statuses = [\n",
        "    \"Resolved\", \"Unresolved\", \"In Progress\", \"Pending\", \"Escalated\",\n",
        "    \"Resolved\", \"Unresolved\", \"In Progress\", \"Pending\", \"Escalated\"\n",
        "]\n",
        "leads = [\n",
        "    \"Lead1001\", \"Lead1002\", \"Lead1003\", \"Lead1004\", \"Lead1005\",\n",
        "    \"Lead1006\", \"Lead1007\", \"Lead1008\", \"Lead1009\", \"Lead1010\"\n",
        "]\n",
        "opportunities = [\n",
        "    \"Opp2001\", \"Opp2002\", \"Opp2003\", \"Opp2004\", \"Opp2005\",\n",
        "    \"Opp2006\", \"Opp2007\", \"Opp2008\", \"Opp2009\", \"Opp2010\"\n",
        "]\n",
        "campaigns = [\n",
        "    \"Camp3001\", \"Camp3002\", \"Camp3003\", \"Camp3004\", \"Camp3005\",\n",
        "    \"Camp3006\", \"Camp3007\", \"Camp3008\", \"Camp3009\", \"Camp3010\"\n",
        "]\n",
        "discount_codes = [\n",
        "    \"DISC10\", \"DISC20\", \"DISC30\", \"DISC40\", \"DISC50\",\n",
        "    \"DISC60\", \"DISC70\", \"DISC80\", \"DISC90\", \"DISC100\"\n",
        "]\n",
        "custom1 = [\n",
        "    \"CustomEntity1\", \"CustomEntity2\", \"CustomEntity3\", \"CustomEntity4\", \"CustomEntity5\",\n",
        "    \"CustomEntity6\", \"CustomEntity7\", \"CustomEntity8\", \"CustomEntity9\", \"CustomEntity10\"\n",
        "]\n",
        "custom2 = [\n",
        "    \"CustomEntityA\", \"CustomEntityB\", \"CustomEntityC\", \"CustomEntityD\", \"CustomEntityE\",\n",
        "    \"CustomEntityF\", \"CustomEntityG\", \"CustomEntityH\", \"CustomEntityI\", \"CustomEntityJ\"\n",
        "]\n",
        "username = [\n",
        "    \"user123\", \"admin456\", \"guest789\", \"member012\", \"user345\",\n",
        "    \"admin678\", \"guest901\", \"member234\", \"user567\", \"admin890\"\n",
        "]\n",
        "address_line1 = [\n",
        "    \"123 Maple Street\", \"456 Oak Avenue\", \"789 Pine Road\", \"321 Birch Lane\", \"654 Cedar Blvd\",\n",
        "    \"987 Spruce Drive\", \"213 Elm Street\", \"546 Ash Avenue\", \"879 Fir Road\", \"132 Willow Lane\"\n",
        "]\n",
        "city = locations.copy()  # Assuming city aligns with locations\n",
        "state = [\n",
        "    \"NY\", \"CA\", \"IL\", \"TX\", \"AZ\",\n",
        "    \"MA\", \"WA\", \"CO\", \"FL\", \"NJ\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "u_DebwEoL1Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 'O' label\n",
        "O_label = \"O\"\n",
        "\n",
        "# Comprehensive entity_label_map with initial mappings\n",
        "entity_label_map = {\n",
        "    \"person\": {\"B\": \"B-PER\", \"I\": \"I-PER\"},\n",
        "    \"alias\": {\"B\": \"B-ALIAS\", \"I\": \"I-ALIAS\"},\n",
        "    \"title\": {\"B\": \"B-TITLE\", \"I\": \"I-TITLE\"},\n",
        "    \"role\": {\"B\": \"B-ROLE\", \"I\": \"I-ROLE\"},\n",
        "    \"organization\": {\"B\": \"B-ORG\", \"I\": \"I-ORG\"},\n",
        "    \"business_name\": {\"B\": \"B-BUS\", \"I\": \"I-BUS\"},\n",
        "    \"business_id\": {\"B\": \"B-BUSID\", \"I\": \"I-BUSID\"},\n",
        "    \"location\": {\"B\": \"B-LOC\", \"I\": \"I-LOC\"},\n",
        "    \"ip\": {\"B\": \"B-IP\", \"I\": \"I-IP\"},\n",
        "    \"mac_address\": {\"B\": \"B-MAC\", \"I\": \"I-MAC\"},\n",
        "    \"project\": {\"B\": \"B-PROJ\", \"I\": \"I-PROJ\"},\n",
        "    \"date\": {\"B\": \"B-DATE\", \"I\": \"I-DATE\"},\n",
        "    \"time\": {\"B\": \"B-TIME\", \"I\": \"I-TIME\"},\n",
        "    \"duration\": {\"B\": \"B-DUR\", \"I\": \"I-DUR\"},\n",
        "    \"event\": {\"B\": \"B-EVENT\", \"I\": \"I-EVENT\"},\n",
        "    \"email\": {\"B\": \"B-EMAIL\", \"I\": \"I-EMAIL\"},\n",
        "    \"phone\": {\"B\": \"B-PHONE\", \"I\": \"I-PHONE\"},\n",
        "    \"url\": {\"B\": \"B-URL\", \"I\": \"I-URL\"},\n",
        "    \"device\": {\"B\": \"B-DEV\", \"I\": \"I-DEV\"},\n",
        "    \"device_id\": {\"B\": \"B-DEV_ID\", \"I\": \"I-DEV_ID\"},\n",
        "    \"password\": {\"B\": \"B-PASS\", \"I\": \"I-PASS\"},\n",
        "    \"access_key\": {\"B\": \"B-KEY\", \"I\": \"I-KEY\"},\n",
        "    \"social_security\": {\"B\": \"B-SSN\", \"I\": \"I-SSN\"},\n",
        "    \"credit_card\": {\"B\": \"B-CC\", \"I\": \"I-CC\"},\n",
        "    \"bank_account\": {\"B\": \"B-BANK\", \"I\": \"I-BANK\"},\n",
        "    \"license_plate\": {\"B\": \"B-PLATE\", \"I\": \"I-PLATE\"},\n",
        "    \"hazmat\": {\"B\": \"B-HAZMAT\", \"I\": \"I-HAZMAT\"},\n",
        "    \"money\": {\"B\": \"B-MONEY\", \"I\": \"I-MONEY\"},\n",
        "    \"currency\": {\"B\": \"B-CUR\", \"I\": \"I-CUR\"},\n",
        "    \"invoice\": {\"B\": \"B-INVOICE\", \"I\": \"I-INVOICE\"},\n",
        "    \"transaction\": {\"B\": \"B-TRANS\", \"I\": \"I-TRANS\"},\n",
        "    \"account\": {\"B\": \"B-ACCT\", \"I\": \"I-ACCT\"},\n",
        "    \"ticket_id\": {\"B\": \"B-TICKET\", \"I\": \"I-TICKET\"},\n",
        "    \"issue_type\": {\"B\": \"B-ISSUE\", \"I\": \"I-ISSUE\"},\n",
        "    \"priority\": {\"B\": \"B-PRIORITY\", \"I\": \"I-PRIORITY\"},\n",
        "    \"resolution_status\": {\"B\": \"B-STATUS\", \"I\": \"I-STATUS\"},\n",
        "    \"lead\": {\"B\": \"B-LEAD\", \"I\": \"I-LEAD\"},\n",
        "    \"opportunity\": {\"B\": \"B-OPP\", \"I\": \"I-OPP\"},\n",
        "    \"campaign\": {\"B\": \"B-CAMP\", \"I\": \"I-CAMP\"},\n",
        "    \"discount_code\": {\"B\": \"B-DISC\", \"I\": \"I-DISC\"},\n",
        "    \"custom1\": {\"B\": \"B-CUST1\", \"I\": \"I-CUST1\"},\n",
        "    \"custom2\": {\"B\": \"B-CUST2\", \"I\": \"I-CUST2\"},\n",
        "    \"username\": {\"B\": \"B-USERNAME\", \"I\": \"I-USERNAME\"},\n",
        "    \"address_line1\": {\"B\": \"B-ADDR1\", \"I\": \"I-ADDR1\"},\n",
        "    \"city\": {\"B\": \"B-CITY\", \"I\": \"I-CITY\"},\n",
        "    \"state\": {\"B\": \"B-STATE\", \"I\": \"I-STATE\"},\n",
        "    # Add other labels as needed\n",
        "}\n"
      ],
      "metadata": {
        "id": "qDwD9wBiQESi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define anomaly and normal templates\n",
        "anomaly_scenarios = [\n",
        "    \"Unauthorized login attempt detected for user {username} from IP {ip}.\",\n",
        "    \"Suspicious activity from IP address {ip} detected in {city}.\",\n",
        "    \"Multiple failed login attempts for {username} on device {device}.\",\n",
        "    \"Unexpected shutdown of the main {organization} server in {location}.\",\n",
        "    \"{username} reported a security breach in the {organization} affecting project {project}.\",\n",
        "    \"Intrusion detected in the {location} server room by IP {ip}.\",\n",
        "    \"Anomaly detected: unusual access patterns from IP {ip} targeting {project}.\",\n",
        "    \"Alert: {username} accessed restricted data without authorization from {city}.\",\n",
        "    \"System compromised: {organization} data integrity at risk due to {device}.\",\n",
        "    \"Abnormal behavior observed from user {username} at {city} accessing {url}.\",\n",
        "    \"Security alert: {username} attempted unauthorized access to {business_name} using {access_key}.\",\n",
        "    \"Hazmat spill reported at {address_line1}, {city}, {state} by {username}.\",\n",
        "    \"Emergency response initiated for {hazmat} incident at {location}.\",\n",
        "    \"Data leak detected involving {credit_card} from {device}.\",\n",
        "    \"{username} changed their password using device {device_id} from IP {ip}.\",\n",
        "    \"Multiple transactions flagged: {transaction} from {bank_account}.\",\n",
        "    \"Invalid access key {access_key} used by {username} from {ip}.\",\n",
        "    \"{username}'s social security number {social_security} was exposed during {event}.\",\n",
        "    \"License plate {license_plate} associated with unauthorized entry at {location}.\",\n",
        "    \"Customer {username} reported issue type {issue_type} with ticket {ticket_id}.\",\n",
        "    \"Anomaly in financial report: {money} discrepancy detected in {account}.\"\n",
        "]\n",
        "\n",
        "normal_scenarios = [\n",
        "    \"{username} accessed the secure server from IP {ip}.\",\n",
        "    \"The server located in {city} was rebooted at {date} {time}.\",\n",
        "    \"{username} updated their password successfully from device {device}.\",\n",
        "    \"System maintenance scheduled in {city} on {date} for {duration}.\",\n",
        "    \"Backup completed successfully for {project} using {device}.\",\n",
        "    \"{username} joined the {organization} team as a {role}.\",\n",
        "    \"{username} left the {organization}.\",\n",
        "    \"New project {project} has been initiated by {username}.\",\n",
        "    \"Meeting scheduled with {username} in {city} on {date} at {time}.\",\n",
        "    \"{username} submitted the quarterly report to {organization} via {url}.\",\n",
        "    \"{username} received an invoice {invoice} for project {project}.\",\n",
        "    \"Transaction {transaction} of {money} approved for account {account}.\",\n",
        "    \"Marketing campaign {campaign} launched with discount code {discount_code}.\",\n",
        "    \"Customer support ticket {ticket_id} assigned to {username} with priority {priority}.\",\n",
        "    \"{username} attended the {event} held at {location}.\",\n",
        "    \"Sales opportunity {opportunity} created by {username} in {campaign}.\",\n",
        "    \"{username} updated contact information including email {email} and phone {phone}.\",\n",
        "    \"Finance department reconciled bank account {bank_account} with transactions {transaction}.\",\n",
        "    \"{username} accessed CRM system using username {username} and device ID {device_id}.\",\n",
        "    \"Lead {lead} converted to opportunity {opportunity} by {username}.\",\n",
        "    \"HR team updated employee {username}'s role to {role}.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "yv85crfNQGO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer with ModernBERT\n",
        "model_name = \"answerdotai/modernbert-base\"  # Replace with \"answerdotai/modernbert-base\" if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "0mcqORM6QH5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(anomaly=False, entity_lists=None, tokenizer=None, entity_label_map=None, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Generates a single synthetic data sample.\n",
        "\n",
        "    Args:\n",
        "        anomaly (bool): Whether to generate an anomaly sample.\n",
        "        entity_lists (dict): Dictionary containing all entity lists.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing 'text', 'ner_labels', 'anomaly_label', and 'entities'.\n",
        "    \"\"\"\n",
        "    if entity_lists is None:\n",
        "        entity_lists = {}\n",
        "\n",
        "    if entity_label_map is None:\n",
        "        raise ValueError(\"entity_label_map must be provided\")\n",
        "\n",
        "    if tokenizer is None:\n",
        "        raise ValueError(\"tokenizer must be provided\")\n",
        "\n",
        "    if anomaly:\n",
        "        template = random.choice(anomaly_scenarios)\n",
        "        anomaly_label = 1\n",
        "    else:\n",
        "        template = random.choice(normal_scenarios)\n",
        "        anomaly_label = 0\n",
        "\n",
        "    # Find all placeholders in the template\n",
        "    placeholders = re.findall(r\"\\{(.*?)\\}\", template)\n",
        "    unique_placeholders = list(set(placeholders))\n",
        "\n",
        "    # Initialize entity selections\n",
        "    selected_entities = {}\n",
        "\n",
        "    # Select entities for each placeholder type\n",
        "    for placeholder in unique_placeholders:\n",
        "        if placeholder in entity_lists and len(entity_lists[placeholder]) > 0:\n",
        "            selected_entities[placeholder] = random.choice(entity_lists[placeholder])\n",
        "        else:\n",
        "            selected_entities[placeholder] = \"Unknown\"  # Fallback for undefined placeholders\n",
        "\n",
        "    # Replace placeholders with selected entities\n",
        "    filled_text = template\n",
        "    for placeholder, entity in selected_entities.items():\n",
        "        filled_text = filled_text.replace(f\"{{{placeholder}}}\", entity)\n",
        "\n",
        "    # Prepare entities list with character offsets\n",
        "    entities = []\n",
        "    for placeholder, entity in selected_entities.items():\n",
        "        # Find all occurrences of the entity in text to handle multiple instances\n",
        "        start_indices = [m.start() for m in re.finditer(re.escape(entity), filled_text)]\n",
        "        for start_char in start_indices:\n",
        "            end_char = start_char + len(entity)\n",
        "            entities.append({\n",
        "                \"text\": entity,\n",
        "                \"type\": placeholder,\n",
        "                \"start_char\": start_char,\n",
        "                \"end_char\": end_char\n",
        "            })\n",
        "\n",
        "    # Tokenize and assign labels\n",
        "    ner_labels = tokenize_and_align_labels(filled_text, entities, tokenizer, entity_label_map, O_label)\n",
        "\n",
        "    return {\n",
        "        \"text\": filled_text,\n",
        "        \"ner_labels\": ner_labels,\n",
        "        \"anomaly_label\": anomaly_label,\n",
        "        \"entities\": entities  # Include entities for validation\n",
        "    }\n",
        "\n",
        "def tokenize_and_align_labels(text, entities, tokenizer, entity_label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Tokenizes the text and aligns the NER labels with the tokenized output using character offsets.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        entities (List[Dict]): A list of entities with 'text', 'type', 'start_char', and 'end_char'.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of BIO labels aligned with the tokenized text.\n",
        "    \"\"\"\n",
        "    # Initialize labels as 'O'\n",
        "    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "    offsets = encoding['offset_mapping']\n",
        "    labels = [O_label] * len(encoding['input_ids'])\n",
        "\n",
        "    # Process each entity\n",
        "    for entity in entities:\n",
        "        entity_type = entity[\"type\"].lower()\n",
        "        start_char = entity[\"start_char\"]\n",
        "        end_char = entity[\"end_char\"]\n",
        "        # Assign labels to tokens\n",
        "        for idx, (token_start, token_end) in enumerate(offsets):\n",
        "            if token_start >= end_char:\n",
        "                break\n",
        "            if token_end <= start_char:\n",
        "                continue\n",
        "            if token_start >= start_char and token_end <= end_char:\n",
        "                if token_start == start_char:\n",
        "                    label_key = entity_label_map.get(entity_type, {}).get(\"B\", O_label)\n",
        "                    labels[idx] = label_key\n",
        "                else:\n",
        "                    label_key = entity_label_map.get(entity_type, {}).get(\"I\", O_label)\n",
        "                    labels[idx] = label_key\n",
        "\n",
        "    return labels\n",
        "\n",
        "def generate_dataset(num_samples=5000, anomaly_ratio=0.3, seed=None, entity_lists=None, tokenizer=None, entity_label_map=None, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): Total number of samples to generate.\n",
        "        anomaly_ratio (float): Proportion of samples that are anomalies.\n",
        "        seed (int, optional): Random seed for reproducibility.\n",
        "        entity_lists (dict): Dictionary containing all entity lists.\n",
        "        tokenizer: The tokenizer instance.\n",
        "        entity_label_map (dict): Mapping for entity labels.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of synthetic data samples.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    dataset = []\n",
        "    for _ in tqdm(range(num_samples), desc=\"Generating Synthetic Data\"):\n",
        "        is_anomaly = random.random() < anomaly_ratio\n",
        "        sample = generate_sample(\n",
        "            anomaly=is_anomaly,\n",
        "            entity_lists=entity_lists,\n",
        "            tokenizer=tokenizer,\n",
        "            entity_label_map=entity_label_map,\n",
        "            O_label=O_label\n",
        "        )\n",
        "        dataset.append(sample)\n",
        "\n",
        "    # Shuffle the dataset to mix anomaly and normal samples\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "EZbWDzFJMrkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all entities for easy access\n",
        "entity_lists = {\n",
        "    \"person\": persons,\n",
        "    \"alias\": aliases,\n",
        "    \"title\": titles,\n",
        "    \"role\": roles,\n",
        "    \"organization\": organizations,\n",
        "    \"business_name\": business_names,\n",
        "    \"business_id\": business_ids,\n",
        "    \"location\": locations,\n",
        "    \"ip\": ips,\n",
        "    \"mac_address\": mac_addresses,\n",
        "    \"project\": projects,\n",
        "    \"date\": dates,\n",
        "    \"time\": times,\n",
        "    \"duration\": durations,\n",
        "    \"event\": events,\n",
        "    \"email\": emails,\n",
        "    \"phone\": phones,\n",
        "    \"url\": urls,\n",
        "    \"device\": devices,\n",
        "    \"device_id\": device_ids,\n",
        "    \"password\": passwords,\n",
        "    \"access_key\": access_keys,\n",
        "    \"social_security\": social_security_numbers,\n",
        "    \"credit_card\": credit_cards,\n",
        "    \"bank_account\": bank_accounts,\n",
        "    \"license_plate\": license_plates,\n",
        "    \"hazmat\": hazmats,\n",
        "    \"money\": money,\n",
        "    \"currency\": currencies,\n",
        "    \"invoice\": invoices,\n",
        "    \"transaction\": transactions,\n",
        "    \"account\": accounts,\n",
        "    \"ticket_id\": ticket_ids,\n",
        "    \"issue_type\": issue_types,\n",
        "    \"priority\": priorities,\n",
        "    \"resolution_status\": resolution_statuses,\n",
        "    \"lead\": leads,\n",
        "    \"opportunity\": opportunities,\n",
        "    \"campaign\": campaigns,\n",
        "    \"discount_code\": discount_codes,\n",
        "    \"custom1\": custom1,\n",
        "    \"custom2\": custom2,\n",
        "    \"username\": username,\n",
        "    \"address_line1\": address_line1,\n",
        "    \"city\": city,\n",
        "    \"state\": state\n",
        "}\n",
        "\n",
        "# Generate synthetic training data\n",
        "synthetic_training_data = generate_dataset(\n",
        "    num_samples=5000,\n",
        "    anomaly_ratio=0.3,\n",
        "    seed=42,\n",
        "    entity_lists=entity_lists,\n",
        "    tokenizer=tokenizer,\n",
        "    entity_label_map=entity_label_map,\n",
        "    O_label=O_label\n",
        ")\n",
        "\n",
        "# Display first 5 samples for verification\n",
        "for i, sample in enumerate(synthetic_training_data[:5], 1):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"Text: {sample['text']}\")\n",
        "    print(f\"NER Labels: {sample['ner_labels']}\")\n",
        "    print(f\"Anomaly Label: {sample['anomaly_label']}\\n\")\n"
      ],
      "metadata": {
        "id": "Udwj6iw5QNwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label_to_id mapping\n",
        "def create_label_to_id_map(label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Creates a mapping from label strings to unique integer IDs.\n",
        "\n",
        "    Args:\n",
        "        label_map (Dict[str, Dict[str, str]]): Updated entity label map.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Mapping from label strings to unique IDs.\n",
        "    \"\"\"\n",
        "    unique_labels = set()\n",
        "    for entity, sub_map in label_map.items():\n",
        "        if isinstance(sub_map, dict):\n",
        "            unique_labels.update(sub_map.values())\n",
        "        else:\n",
        "            unique_labels.add(sub_map)  # Handle 'O' if present\n",
        "\n",
        "    unique_labels.add(O_label)  # Ensure 'O' is included\n",
        "\n",
        "    sorted_labels = sorted(unique_labels)  # Sorting for consistency\n",
        "    label_to_id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
        "    return label_to_id\n",
        "\n",
        "# Create the label_to_id mapping\n",
        "label_to_id = create_label_to_id_map(entity_label_map, O_label=O_label)\n",
        "print(\"Label to ID mapping:\", label_to_id)\n"
      ],
      "metadata": {
        "id": "MODyjrx6Mv9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label_to_id mapping\n",
        "def create_label_to_id_map(label_map, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Creates a mapping from label strings to unique integer IDs.\n",
        "\n",
        "    Args:\n",
        "        label_map (Dict[str, Dict[str, str]]): Updated entity label map.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Mapping from label strings to unique IDs.\n",
        "    \"\"\"\n",
        "    unique_labels = set()\n",
        "    for entity, sub_map in label_map.items():\n",
        "        if isinstance(sub_map, dict):\n",
        "            unique_labels.update(sub_map.values())\n",
        "        else:\n",
        "            unique_labels.add(sub_map)  # Handle 'O' if present\n",
        "\n",
        "    unique_labels.add(O_label)  # Ensure 'O' is included\n",
        "\n",
        "    sorted_labels = sorted(unique_labels)  # Sorting for consistency\n",
        "    label_to_id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
        "    return label_to_id\n",
        "\n",
        "# Create the label_to_id mapping\n",
        "label_to_id = create_label_to_id_map(entity_label_map, O_label=O_label)\n",
        "print(\"Label to ID mapping:\", label_to_id)\n"
      ],
      "metadata": {
        "id": "-NZqgdQsNgBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointNERAnomalyDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, label_to_id, max_length=128):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data (List[Dict]): The synthetic dataset.\n",
        "            tokenizer: The tokenizer instance.\n",
        "            label_to_id (Dict[str, int]): Mapping from label strings to IDs.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_to_id = label_to_id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        ner_labels = item['ner_labels']\n",
        "        anomaly_label = item['anomaly_label']\n",
        "        entities = item.get('entities', [])  # Retrieve entities if available\n",
        "\n",
        "        # Tokenize the input text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt',\n",
        "            is_split_into_words=False\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        offsets = encoding['offset_mapping'].squeeze().tolist()\n",
        "\n",
        "        # Initialize labels as 'O'\n",
        "        labels = [O_label] * len(input_ids)\n",
        "\n",
        "        # Assign labels based on entities\n",
        "        for entity in entities:\n",
        "            entity_type = entity[\"type\"].lower()  # Ensure lowercase for consistency\n",
        "            start_char = entity[\"start_char\"]\n",
        "            end_char = entity[\"end_char\"]\n",
        "            for idx_token, (token_start, token_end) in enumerate(offsets):\n",
        "                if token_start >= end_char:\n",
        "                    break\n",
        "                if token_end <= start_char:\n",
        "                    continue\n",
        "                if token_start >= start_char and token_end <= end_char:\n",
        "                    if token_start == start_char:\n",
        "                        labels[idx_token] = entity_label_map[entity_type][\"B\"]\n",
        "                    else:\n",
        "                        labels[idx_token] = entity_label_map[entity_type][\"I\"]\n",
        "\n",
        "        # Convert labels to IDs\n",
        "        ner_label_ids = [self.label_to_id.get(label, self.label_to_id[O_label]) for label in labels]\n",
        "\n",
        "        # Handle padding labels\n",
        "        if len(ner_label_ids) < self.max_length:\n",
        "            ner_label_ids += [self.label_to_id[O_label]] * (self.max_length - len(ner_label_ids))\n",
        "        elif len(ner_label_ids) > self.max_length:\n",
        "            ner_label_ids = ner_label_ids[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'ner_labels': torch.tensor(ner_label_ids, dtype=torch.long),\n",
        "            'anomaly_labels': torch.tensor(anomaly_label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "CpP8lqKFMyHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataset\n",
        "combined_dataset = JointNERAnomalyDataset(\n",
        "    data=synthetic_training_data,\n",
        "    tokenizer=tokenizer,\n",
        "    label_to_id=label_to_id,\n",
        "    max_length=128  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Split into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(combined_dataset))\n",
        "val_size = len(combined_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "of5HHdrmQbn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "GD3t1lvYQeFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the DataLoader to verify no KeyErrors occur\n",
        "try:\n",
        "    for batch in train_loader:\n",
        "        print(\"Batch successfully loaded:\")\n",
        "        print({\n",
        "            'input_ids': batch['input_ids'].shape,\n",
        "            'attention_mask': batch['attention_mask'].shape,\n",
        "            'ner_labels': batch['ner_labels'].shape,\n",
        "            'anomaly_labels': batch['anomaly_labels'].shape\n",
        "        })\n",
        "        break  # Only verify the first batch\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError encountered: {e}. Please ensure all entity types are mapped.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "FnQc6Y4QQimT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class JointNERAnomalyModel(nn.Module):\n",
        "    def __init__(self, base_model, num_ner_labels, num_anomaly_labels):\n",
        "        super(JointNERAnomalyModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.hidden_size = base_model.config.hidden_size\n",
        "\n",
        "        # NER head\n",
        "        self.ner_classifier = nn.Linear(self.hidden_size, num_ner_labels)\n",
        "\n",
        "        # Anomaly Detection head\n",
        "        self.anomaly_classifier = nn.Linear(self.hidden_size, num_anomaly_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state  # For NER\n",
        "\n",
        "        # Use the CLS token's hidden state for anomaly classification\n",
        "        cls_output = sequence_output[:, 0, :]  # [batch_size, hidden_size]\n",
        "\n",
        "        ner_logits = self.ner_classifier(sequence_output)\n",
        "        anomaly_logits = self.anomaly_classifier(cls_output)\n",
        "\n",
        "        return ner_logits, anomaly_logits\n",
        "\n"
      ],
      "metadata": {
        "id": "9qPjcEjzQlCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ModernBERT model\n",
        "base_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Define number of labels\n",
        "num_ner_labels = len(label_to_id)  # Total unique NER labels\n",
        "num_anomaly_labels = 2  # Binary classification: Normal or Anomaly\n",
        "\n",
        "# Initialize the joint model\n",
        "model = JointNERAnomalyModel(base_model, num_ner_labels, num_anomaly_labels)\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(device)\n",
        "\n",
        "# Define loss functions\n",
        "ner_loss_fn = nn.CrossEntropyLoss(ignore_index=label_to_id[O_label])  # Ignore 'O' label in loss\n",
        "anomaly_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "id": "Gbvsab89QnLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 3  # Adjust based on your requirements and Colab's runtime limits\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ],
      "metadata": {
        "id": "8CScOIZ7QrIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        ner_labels = batch['ner_labels'].to(device)\n",
        "        anomaly_labels = batch['anomaly_labels'].to(device)\n",
        "\n",
        "        ner_logits, anomaly_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute NER loss\n",
        "        ner_loss = ner_loss_fn(ner_logits.view(-1, num_ner_labels), ner_labels.view(-1))\n",
        "\n",
        "        # Compute Anomaly Detection loss\n",
        "        anomaly_loss = anomaly_loss_fn(anomaly_logits, anomaly_labels)\n",
        "\n",
        "        # Total loss\n",
        "        loss = ner_loss + anomaly_loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "id": "JQLwLZkGQtFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, label_to_id, O_label=\"O\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        dataloader: DataLoader for evaluation.\n",
        "        label_to_id: Dictionary mapping labels to IDs.\n",
        "        O_label (str): The label for non-entity tokens.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ner_preds = []\n",
        "    ner_true = []\n",
        "    anomaly_preds = []\n",
        "    anomaly_true = []\n",
        "\n",
        "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            ner_labels = batch['ner_labels'].to(device)\n",
        "            anomaly_labels = batch['anomaly_labels'].to(device)\n",
        "\n",
        "            ner_logits, anomaly_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # NER Predictions\n",
        "            ner_pred = torch.argmax(ner_logits, dim=-1).cpu().numpy()\n",
        "            ner_true_labels = ner_labels.cpu().numpy()\n",
        "            ner_preds.extend(ner_pred.flatten())\n",
        "            ner_true.extend(ner_true_labels.flatten())\n",
        "\n",
        "            # Anomaly Predictions\n",
        "            anomaly_pred = torch.argmax(anomaly_logits, dim=-1).cpu().numpy()\n",
        "            anomaly_true_labels = anomaly_labels.cpu().numpy()\n",
        "            anomaly_preds.extend(anomaly_pred)\n",
        "            anomaly_true.extend(anomaly_true_labels)\n",
        "\n",
        "    # Remove padding tokens ('O' label)\n",
        "    o_label_id = label_to_id.get(O_label, -1)\n",
        "    valid_indices = [i for i, label in enumerate(ner_true) if label != o_label_id]\n",
        "    filtered_ner_preds = [ner_preds[i] for i in valid_indices]\n",
        "    filtered_ner_true = [ner_true[i] for i in valid_indices]\n",
        "\n",
        "    # Map label IDs back to labels\n",
        "    filtered_ner_preds_labels = [id_to_label.get(id, O_label) for id in filtered_ner_preds]\n",
        "    filtered_ner_true_labels = [id_to_label.get(id, O_label) for id in filtered_ner_true]\n",
        "\n",
        "    # NER Classification Report\n",
        "    print(\"NER Classification Report:\")\n",
        "    print(classification_report(filtered_ner_true_labels, filtered_ner_preds_labels, digits=4))\n",
        "\n",
        "    # Anomaly Detection Classification Report\n",
        "    print(\"Anomaly Detection Classification Report:\")\n",
        "    print(classification_report(anomaly_true, anomaly_preds, digits=4))\n",
        "\n",
        "    # Plot Confusion Matrix for Anomaly Detection\n",
        "    plt.figure(figsize=(6,6))\n",
        "    cm = confusion_matrix(anomaly_true, anomaly_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Anomaly Detection Confusion Matrix')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZtDJZO_tQ0AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "model_dir = \"/content/drive/MyDrive/models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the model to Google Drive\n",
        "torch.save(model.state_dict(), os.path.join(model_dir, \"joint_ner_anomaly_model_sm.pth\"))\n",
        "print(f\"Model saved to {model_dir}/joint_ner_anomaly_model.pth\")\n"
      ],
      "metadata": {
        "id": "mT0G35p0SIwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}