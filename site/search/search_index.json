{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <p>This documentation and repo contain</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"preferences/","title":"User Preferences","text":"<p>Welcome to the user preferences page. Here, you can toggle between light and dark modes for the site.</p> <p>Click the button below to switch themes:</p> <p> </p> <p>For more customization, explore additional settings on this page.</p>"},{"location":"base/","title":"\ud83d\ude80 ModernBERT Base Model","text":"<p>Welcome to the comprehensive documentation for the Joint Named Entity Recognition (NER) and Anomaly Detection experiment using ModernBERT. This guide is designed to help you set up, train, and utilize two versions of the model within a Google Colab environment. Whether you're a beginner or an experienced practitioner, this documentation provides the necessary information to effectively conduct your experiments and achieve state-of-the-art results.</p>"},{"location":"base/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Use Case</li> <li>Introduction</li> <li>Model Versions</li> <li>Small Model (149MB)</li> <li>Large Model (395MB)</li> <li>Setting Up Google Colab</li> <li>Creating a New Notebook</li> <li>Configuring Secrets</li> <li>Running the Experiment</li> <li>Installing Dependencies</li> <li>Initializing the Tokenizer and Model</li> <li>Training the Model</li> <li>Saving Models to Google Drive</li> <li>Inference and Usage</li> <li>Troubleshooting</li> <li>References</li> <li>Conclusion</li> </ol>"},{"location":"base/#introduction","title":"Introduction","text":""},{"location":"base/#purpose-of-modernbert","title":"\ud83c\udf1f Purpose of ModernBERT","text":"<p>The primary objective of utilizing ModernBERT in this experiment is to establish a robust foundation for conducting future experiments in Named Entity Recognition and Anomaly Detection. ModernBERT provides users with the flexibility to train, fine-tune, and apply various advanced techniques inherent to transformer models. Leveraging Google Colab, this setup ensures accessibility and cost-effectiveness, allowing anyone to perform complex tasks without the need for high-end local hardware.</p>"},{"location":"base/#key-features","title":"\ud83d\udd27 Key Features","text":"<ul> <li>Dual Model Versions: Train both small and large versions to balance performance and resource utilization.</li> <li>Google Colab Integration: Utilize free computational resources for training and experimentation.</li> <li>Hugging Face Compatibility: Seamlessly download and deploy models through Hugging Face.</li> <li>Scalable Experimentation: Easily switch between models and configurations to explore different approaches.</li> </ul>"},{"location":"base/#accessibility","title":"\ud83c\udf10 Accessibility","text":"<p>ModernBERT is freely accessible via Google Colab, enabling users to:</p> <ul> <li>Train models without incurring hardware costs.</li> <li>Download trained models for local deployment.</li> <li>Perform inference using Hugging Face's free services.</li> </ul> <p>This democratizes access to advanced NLP capabilities, fostering a platform for widespread experimentation and innovation.</p> <p>For detailed instructions on setting up your environment, refer to the Setting Up Google Colab section.</p>"},{"location":"base/#key-takeaways","title":"\ud83c\udfaf Key Takeaways","text":"<p>This documentation provides a thorough guide to setting up, training, and utilizing the Joint Named Entity Recognition (NER) and Anomaly Detection model using ModernBERT in a Google Colab environment. By following the structured steps outlined in each section, you can effectively manage both small and large model versions, ensuring flexibility and scalability for your projects.</p> <p>Key Highlights:</p> <ul> <li>Dual Model Versions: Train both small (149MB) and large (395MB) versions to balance performance and resource utilization.</li> <li>Secure and Accessible: Utilize Google Colab's free computational resources and secure secrets management to streamline experimentation.</li> <li>Comprehensive Training Procedures: Detailed instructions cover everything from dependency installation to model training and saving.</li> <li>Robust Inference Pipeline: Ready-to-use functions facilitate seamless deployment and real-world application of the trained models.</li> <li>Proactive Troubleshooting: Address common issues with provided solutions to maintain a smooth workflow.</li> </ul>"},{"location":"base/#future-enhancements","title":"\ud83c\udf31 Future Enhancements","text":"<p>To achieve state-of-the-art (SOTA) performance, consider implementing the following strategies:</p> <ol> <li>Advanced Tokenization:</li> <li> <p>Customize tokenizers to better handle domain-specific terminology and entities.</p> </li> <li> <p>Entity-Aware Embeddings:</p> </li> <li> <p>Incorporate mechanisms that give special attention to entity tokens, enhancing model focus on critical information.</p> </li> <li> <p>Multi-Task Learning:</p> </li> <li> <p>Explore more sophisticated multi-task learning strategies to optimize shared representations and improve overall performance.</p> </li> <li> <p>Contrastive Learning:</p> </li> <li> <p>Implement contrastive learning techniques to enhance anomaly detection accuracy by differentiating normal and anomalous patterns more effectively.</p> </li> <li> <p>Ensemble Models:</p> </li> <li> <p>Combine multiple model architectures to leverage their collective strengths, potentially boosting performance.</p> </li> <li> <p>Hyperparameter Optimization:</p> </li> <li> <p>Utilize automated tools like Optuna or Hyperopt for fine-tuning hyperparameters, ensuring optimal model performance.</p> </li> <li> <p>Knowledge Distillation:</p> </li> <li> <p>Employ knowledge distillation to transfer knowledge from larger models to smaller ones, enhancing performance without significant computational overhead.</p> </li> <li> <p>Data Augmentation:</p> </li> <li> <p>Expand and diversify your training data using advanced data augmentation techniques to improve model generalization.</p> </li> <li> <p>Regularization Techniques:</p> </li> <li> <p>Apply regularization methods such as dropout, weight decay, and early stopping to prevent overfitting and improve model robustness.</p> </li> <li> <p>Monitoring and Evaluation:</p> <ul> <li>Implement comprehensive monitoring and evaluation frameworks to continuously assess model performance and identify areas for improvement.</li> </ul> </li> </ol> <p>By integrating these strategies, you can elevate the model's performance, pushing it closer to SOTA standards in both NER and Anomaly Detection tasks.</p>"},{"location":"base/#accessibility-and-experimentation","title":"\ud83c\udf10 Accessibility and Experimentation","text":"<p>Leveraging Google Colab ensures that this experiment remains accessible to a broad audience. Users can:</p> <ul> <li>Train Models Freely: Utilize Colab's free computational resources to train both model versions without incurring costs.</li> <li>Download and Deploy Locally: After training, models can be downloaded and run locally, providing flexibility in deployment.</li> <li>Use Hugging Face for Inference: Perform inference using Hugging Face's free services, enabling scalable and efficient deployment solutions.</li> </ul> <p>Emphasizing Experimentation: ModernBERT serves as a solid foundation for ongoing and future experiments, offering the flexibility to explore various training, fine-tuning, and deployment techniques inherent to transformer models.</p>"},{"location":"base/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Hugging Face Transformers: For providing powerful tools that facilitate advanced NLP experiments.</li> <li>PyTorch Community: For robust and flexible deep learning frameworks.</li> <li>Google Colab Team: For offering free and accessible computational resources.</li> <li>ModernBERT Developers: For developing and maintaining the ModernBERT models used in this experiment.</li> <li>OpenAI: For inspiring continuous improvement and innovation in AI research.</li> </ul>"},{"location":"base/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p> <p>Happy Modeling! \ud83d\ude80</p>"},{"location":"base/inference/","title":"Inference and Usage","text":""},{"location":"base/inference/#loading-the-trained-models","title":"\ud83e\udde0 Loading the Trained Models","text":"<p>To utilize the trained models for inference, follow these steps:</p>"},{"location":"base/inference/#for-small-model","title":"For Small Model","text":"<pre><code># Initialize the tokenizer and base model\ntokenizer_sm = AutoTokenizer.from_pretrained(\"answerdotai/modernbert-base\")\nbase_model_sm = AutoModel.from_pretrained(\"answerdotai/modernbert-base\")\n\n# Initialize the joint model\nmodel_sm = JointNERAnomalyModel(base_model_sm, num_ner_labels, num_anomaly_labels)\n\n# Load the saved state dictionary\nmodel_path_sm = \"/content/drive/MyDrive/JointNERAnomalyModel/joint_ner_anomaly_model_sm.pth\"\nmodel_sm.load_state_dict(torch.load(model_path_sm, map_location=device))\nmodel_sm.to(device)\nmodel_sm.eval()\nprint(\"Small model loaded successfully!\")\n</code></pre>"},{"location":"base/inference/#for-large-model","title":"For Large Model","text":"<pre><code># Initialize the tokenizer and base model\ntokenizer_lg = AutoTokenizer.from_pretrained(\"answerdotai/modernbert-large\")\nbase_model_lg = AutoModel.from_pretrained(\"answerdotai/modernbert-large\")\n\n# Initialize the joint model\nmodel_lg = JointNERAnomalyModel(base_model_lg, num_ner_labels, num_anomaly_labels)\n\n# Load the saved state dictionary\nmodel_path_lg = \"/content/drive/MyDrive/JointNERAnomalyModel/joint_ner_anomaly_model_lg.pth\"\nmodel_lg.load_state_dict(torch.load(model_path_lg, map_location=device))\nmodel_lg.to(device)\nmodel_lg.eval()\nprint(\"Large model loaded successfully!\")\n</code></pre>"},{"location":"base/inference/#performing-inference","title":"\ud83d\udd0d Performing Inference","text":"<p>Create a function to perform NER and Anomaly Detection on new text inputs.</p> <pre><code>def predict(text, model, tokenizer, label_to_id, device, max_length=128):\n    \"\"\"\n    Performs NER and Anomaly Detection on the input text.\n\n    Args:\n        text (str): The input text.\n        model: The trained JointNERAnomalyModel.\n        tokenizer: The tokenizer instance.\n        label_to_id (dict): Mapping from label strings to IDs.\n        device: The computation device.\n        max_length (int): Maximum sequence length.\n\n    Returns:\n        Tuple[List[Tuple[str, str]], str]: List of (Entity, Label) and Anomaly Prediction.\n    \"\"\"\n    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        ner_logits, anomaly_logits = model(input_ids, attention_mask)\n\n    # NER Predictions\n    ner_pred = torch.argmax(ner_logits, dim=-1).cpu().numpy()[0]\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    labels = [list(label_to_id.keys())[list(label_to_id.values()).index(l)] for l in ner_pred[:len(tokens)]]\n\n    entities = []\n    current_entity = []\n    current_label = None\n\n    for token, label in zip(tokens, labels):\n        if label.startswith(\"B-\"):\n            if current_entity:\n                entities.append((\" \".join(current_entity), current_label))\n                current_entity = []\n            current_entity.append(token)\n            current_label = label[2:]\n        elif label.startswith(\"I-\") and current_label == label[2:]:\n            current_entity.append(token)\n        else:\n            if current_entity:\n                entities.append((\" \".join(current_entity), current_label))\n                current_entity = []\n            current_label = None\n\n    if current_entity:\n        entities.append((\" \".join(current_entity), current_label))\n\n    # Anomaly Detection Prediction\n    anomaly_pred = torch.argmax(anomaly_logits, dim=-1).cpu().numpy()[0]\n    anomaly_label = \"Anomaly\" if anomaly_pred == 1 else \"Normal\"\n\n    return entities, anomaly_label\n</code></pre>"},{"location":"base/inference/#example-usage","title":"\ud83d\udca1 Example Usage","text":"<p>Perform predictions using both the small and large models.</p> <pre><code># Example text\ntext = \"Unauthorized access detected for user admin456 from IP 192.168.1.1.\"\n\n# Perform prediction using the small model\nentities_sm, anomaly_sm = predict(text, model_sm, tokenizer_sm, label_to_id, device)\nprint(\"Entities (Small Model):\", entities_sm)\nprint(\"Anomaly Detection (Small Model):\", anomaly_sm)\n\n# Perform prediction using the large model\nentities_lg, anomaly_lg = predict(text, model_lg, tokenizer_lg, label_to_id, device)\nprint(\"Entities (Large Model):\", entities_lg)\nprint(\"Anomaly Detection (Large Model):\", anomaly_lg)\n</code></pre> <p>Sample Output:</p> <p>Entities (Small Model): [('admin456', 'USERNAME'), ('192.168.1.1', 'IP')]</p> <p>Anomaly Detection (Small Model): Anomaly</p> <p>Entities (Large Model): [('admin456', 'USERNAME'), ('192.168.1.1', 'IP')]</p> <p>Anomaly Detection (Large Model): Anomaly</p>"},{"location":"base/model_versions/","title":"Model Versions","text":""},{"location":"base/model_versions/#model-versions_1","title":"Model Versions","text":"<p>In this experiment, two versions of the Joint NER and Anomaly Detection model are developed to cater to different computational needs and performance requirements. Below are the detailed specifications of each version.</p>"},{"location":"base/model_versions/#small-model-149mb","title":"Small Model (149MB)","text":"<ul> <li>Model Name: <code>answerdotai/modernbert-base</code></li> <li>Output Filename: <code>joint_ner_anomaly_model_sm.pth</code></li> <li>Description: This version is optimized for faster training and inference, making it suitable for environments with limited computational resources. It strikes a balance between performance and resource utilization.</li> </ul> <p>Key Characteristics:</p> <ul> <li>Size: 149MB</li> <li>Parameters: Fewer parameters leading to quicker training times.</li> <li>Use Cases: Ideal for deployment in resource-constrained settings or when rapid iterations are necessary.</li> </ul> <p>For more details, refer to the Training the Model section.</p>"},{"location":"base/model_versions/#large-model-395mb","title":"Large Model (395MB)","text":"<ul> <li>Model Name: <code>answerdotai/modernbert-large</code></li> <li>Output Filename: <code>joint_ner_anomaly_model_lg.pth</code></li> <li>Description: The larger model offers enhanced performance with a greater number of parameters, suitable for tasks requiring higher accuracy and deeper contextual understanding.</li> </ul> <p>Key Characteristics:</p> <ul> <li>Size: 395MB</li> <li>Parameters: More parameters allow for capturing complex patterns in data.</li> <li>Use Cases: Best suited for scenarios where maximum accuracy is paramount and computational resources are ample.</li> </ul> <p>For more details, refer to the Training the Model section.</p>"},{"location":"base/references/","title":"References","text":""},{"location":"base/references/#references","title":"References","text":""},{"location":"base/references/#libraries-and-frameworks","title":"\ud83d\udcda Libraries and Frameworks","text":"<ul> <li>Hugging Face Transformers:</li> <li>Documentation: https://huggingface.co/transformers/</li> <li> <p>Repository: https://github.com/huggingface/transformers</p> </li> <li> <p>PyTorch:</p> </li> <li>Documentation: https://pytorch.org/docs/stable/index.html</li> <li> <p>Repository: https://github.com/pytorch/pytorch</p> </li> <li> <p>Scikit-Learn:</p> </li> <li>Documentation: https://scikit-learn.org/stable/</li> <li> <p>Repository: https://github.com/scikit-learn/scikit-learn</p> </li> <li> <p>TQDM:</p> </li> <li>Documentation: https://tqdm.github.io/</li> <li> <p>Repository: https://github.com/tqdm/tqdm</p> </li> <li> <p>Seaborn:</p> </li> <li>Documentation: https://seaborn.pydata.org/</li> <li> <p>Repository: https://github.com/mwaskom/seaborn</p> </li> <li> <p>NLTK:</p> </li> <li>Documentation: https://www.nltk.org/</li> <li> <p>Repository: https://github.com/nltk/nltk</p> </li> <li> <p>SpaCy:</p> </li> <li>Documentation: https://spacy.io/</li> <li>Repository: https://github.com/explosion/spaCy</li> </ul>"},{"location":"base/references/#research-papers","title":"\ud83d\udcc4 Research Papers","text":"<ul> <li>ModernBERT Paper: (Replace with the actual link if available)</li> </ul>"},{"location":"base/references/#tools","title":"\ud83d\udee0\ufe0f Tools","text":"<ul> <li>Google Colab:</li> <li> <p>Website: https://colab.research.google.com/</p> </li> <li> <p>MkDocs:</p> </li> <li>Documentation: https://www.mkdocs.org/</li> <li>Repository: https://github.com/mkdocs/mkdocs</li> </ul>"},{"location":"base/run/","title":"Training the Model","text":""},{"location":"base/run/#installing-dependencies","title":"\ud83d\udce6 Installing Dependencies","text":"<p>Begin by installing the necessary libraries required for the experiment. Execute the following commands in separate code cells within your Colab notebook:</p> <p>Install Hugging Face Transformers and other dependencies:</p> <pre><code>pip install transformers\npip install torch\npip install scikit-learn\npip install tqdm\npip install seaborn\npip install nltk\npip install spacy\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"base/run/#importing-libraries","title":"\ud83d\udcda Importing Libraries","text":"<p>Import the required libraries in your notebook:</p> <pre><code>import torch\nfrom torch import nn\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm import tqdm\nimport random\nimport re\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport spacy\n</code></pre> <p>Download NLTK data and set random seeds for reproducibility:</p> <pre><code># Download NLTK data\nnltk.download('wordnet')\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n</code></pre> <p>Expected Output:</p> <p>Using device: cuda</p>"},{"location":"base/run/#initializing-the-tokenizer-and-model","title":"\ud83d\udd27 Initializing the Tokenizer and Model","text":"<p>Depending on the model version you wish to train (small or large), initialize the tokenizer and base model accordingly.</p>"},{"location":"base/run/#for-small-model","title":"For Small Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModel\n\n# Initialize the tokenizer with ModernBERT Base\nmodel_name_sm = \"answerdotai/modernbert-base\"  # Small model\ntokenizer_sm = AutoTokenizer.from_pretrained(model_name_sm)\n\n# Initialize the base model\ntry:\n    base_model_sm = AutoModel.from_pretrained(model_name_sm)\n    print(\"Successfully loaded ModernBERT Base!\")\nexcept ValueError as e:\n    print(f\"Error loading model '{model_name_sm}': {e}\")\n</code></pre>"},{"location":"base/run/#for-large-model","title":"For Large Model","text":"<pre><code>from transformers import AutoTokenizer, AutoModel\n\n# Initialize the tokenizer with ModernBERT Large\nmodel_name_lg = \"answerdotai/modernbert-large\"  # Large model\ntokenizer_lg = AutoTokenizer.from_pretrained(model_name_lg)\n\n# Initialize the base model\ntry:\n    base_model_lg = AutoModel.from_pretrained(model_name_lg)\n    print(\"Successfully loaded ModernBERT Large!\")\nexcept ValueError as e:\n    print(f\"Error loading model '{model_name_lg}': {e}\")\n</code></pre> <p>\ud83d\udd0d Note: Ensure that <code>answerdotai/modernbert-base</code> and <code>answerdotai/modernbert-large</code> are available on Hugging Face's model hub. If not, verify the model names or consult the model provider.</p>"},{"location":"base/run/#defining-the-jointneranomalymodel","title":"\ud83d\udee0\ufe0f Defining the JointNERAnomalyModel","text":"<p>Implement the <code>JointNERAnomalyModel</code> with the corrected forward method to handle the absence of <code>pooler_output</code> by using the <code>[CLS]</code> token for anomaly classification.</p> <pre><code>import torch.nn as nn\n\nclass JointNERAnomalyModel(nn.Module):\n    def __init__(self, base_model, num_ner_labels, num_anomaly_labels):\n        super(JointNERAnomalyModel, self).__init__()\n        self.base_model = base_model\n        self.hidden_size = base_model.config.hidden_size\n\n        # NER classifier: processes all token embeddings\n        self.ner_classifier = nn.Linear(self.hidden_size, num_ner_labels)\n\n        # Anomaly detection classifier: processes CLS token embedding\n        self.anomaly_classifier = nn.Linear(self.hidden_size, num_anomaly_labels)\n\n    def forward(self, input_ids, attention_mask):\n        # Get base model outputs\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Hidden states of all tokens (sequence output)\n        sequence_output = outputs.last_hidden_state\n\n        # Use the CLS token (first token) for anomaly classification\n        cls_output = sequence_output[:, 0, :]  # Shape: [batch_size, hidden_size]\n\n        # NER logits for each token\n        ner_logits = self.ner_classifier(sequence_output)\n\n        # Anomaly logits from the CLS token\n        anomaly_logits = self.anomaly_classifier(cls_output)\n\n        return ner_logits, anomaly_logits\n</code></pre>"},{"location":"base/run/#initializing-the-joint-model","title":"\u2699\ufe0f Initializing the Joint Model","text":"<p>Choose the model version you intend to train and initialize accordingly.</p>"},{"location":"base/run/#for-small-model_1","title":"For Small Model","text":"<pre><code># Define label counts\nnum_ner_labels = len(label_to_id)  # Total unique NER labels\nnum_anomaly_labels = 2  # Binary classification: Normal or Anomaly\n\n# Initialize the joint model with the small base model\nmodel_sm = JointNERAnomalyModel(base_model_sm, num_ner_labels, num_anomaly_labels)\n\n# Move model to GPU\nmodel_sm.to(device)\n</code></pre>"},{"location":"base/run/#for-large-model_1","title":"For Large Model","text":"<pre><code># Define label counts\nnum_ner_labels = len(label_to_id)  # Total unique NER labels\nnum_anomaly_labels = 2  # Binary classification: Normal or Anomaly\n\n# Initialize the joint model with the large base model\nmodel_lg = JointNERAnomalyModel(base_model_lg, num_ner_labels, num_anomaly_labels)\n\n# Move model to GPU\nmodel_lg.to(device)\n</code></pre>"},{"location":"base/run/#defining-loss-functions-and-optimizer","title":"\ud83d\udcc9 Defining Loss Functions and Optimizer","text":"<pre><code># Define loss functions\nner_loss_fn = nn.CrossEntropyLoss(ignore_index=label_to_id[O_label])  # Ignore 'O' label in loss\nanomaly_loss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer_sm = torch.optim.AdamW(model_sm.parameters(), lr=2e-5)\noptimizer_lg = torch.optim.AdamW(model_lg.parameters(), lr=2e-5)\n</code></pre>"},{"location":"base/run/#setting-up-the-learning-rate-scheduler","title":"\u23f1\ufe0f Setting Up the Learning Rate Scheduler","text":"<pre><code>from transformers import get_linear_schedule_with_warmup\n\nepochs = 3  # Adjust based on your requirements and Colab's runtime limits\n\n# For Small Model\ntotal_steps_sm = len(train_loader_sm) * epochs\n\nscheduler_sm = get_linear_schedule_with_warmup(\n    optimizer_sm,\n    num_warmup_steps=0,\n    num_training_steps=total_steps_sm\n)\n\n# For Large Model\ntotal_steps_lg = len(train_loader_lg) * epochs\n\nscheduler_lg = get_linear_schedule_with_warmup(\n    optimizer_lg,\n    num_warmup_steps=0,\n    num_training_steps=total_steps_lg\n)\n</code></pre>"},{"location":"base/run/#training-loop","title":"\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Training Loop","text":""},{"location":"base/run/#for-small-model_2","title":"For Small Model","text":"<pre><code>for epoch in range(epochs):\n    model_sm.train()\n    total_loss = 0\n    for batch in tqdm(train_loader_sm, desc=f\"Training Epoch {epoch+1} - Small Model\"):\n        optimizer_sm.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        ner_labels = batch['ner_labels'].to(device)\n        anomaly_labels = batch['anomaly_labels'].to(device)\n\n        ner_logits, anomaly_logits = model_sm(input_ids, attention_mask)\n\n        # Compute NER loss\n        ner_loss = ner_loss_fn(ner_logits.view(-1, num_ner_labels), ner_labels.view(-1))\n\n        # Compute Anomaly Detection loss\n        anomaly_loss = anomaly_loss_fn(anomaly_logits, anomaly_labels)\n\n        # Total loss\n        loss = ner_loss + anomaly_loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        loss.backward()\n\n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model_sm.parameters(), max_norm=1.0)\n\n        # Optimizer step\n        optimizer_sm.step()\n\n        # Scheduler step\n        scheduler_sm.step()\n\n    avg_loss = total_loss / len(train_loader_sm)\n    print(f\"Epoch {epoch+1} - Small Model Average Loss: {avg_loss}\")\n</code></pre>"},{"location":"base/run/#for-large-model_2","title":"For Large Model","text":"<pre><code>for epoch in range(epochs):\n    model_lg.train()\n    total_loss = 0\n    for batch in tqdm(train_loader_lg, desc=f\"Training Epoch {epoch+1} - Large Model\"):\n        optimizer_lg.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        ner_labels = batch['ner_labels'].to(device)\n        anomaly_labels = batch['anomaly_labels'].to(device)\n\n        ner_logits, anomaly_logits = model_lg(input_ids, attention_mask)\n\n        # Compute NER loss\n        ner_loss = ner_loss_fn(ner_logits.view(-1, num_ner_labels), ner_labels.view(-1))\n\n        # Compute Anomaly Detection loss\n        anomaly_loss = anomaly_loss_fn(anomaly_logits, anomaly_labels)\n\n        # Total loss\n        loss = ner_loss + anomaly_loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        loss.backward()\n\n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model_lg.parameters(), max_norm=1.0)\n\n        # Optimizer step\n        optimizer_lg.step()\n\n        # Scheduler step\n        scheduler_lg.step()\n\n    avg_loss = total_loss / len(train_loader_lg)\n    print(f\"Epoch {epoch+1} - Large Model Average Loss: {avg_loss}\")\n</code></pre> <p>\ud83d\udcc8 Monitoring Training:</p> <ul> <li>Loss Tracking: Observe the average loss per epoch to monitor convergence.</li> <li>TensorBoard Integration: Optionally, integrate TensorBoard for more detailed monitoring.</li> </ul> <p>Refer to the Troubleshooting section for solutions to common training issues.</p>"},{"location":"base/saving_models/","title":"Saving Models to Google Drive","text":""},{"location":"base/saving_models/#saving-trained-models-to-google-drive","title":"\ud83d\udcbe Saving Trained Models to Google Drive","text":"<p>After successfully training your models, it's essential to save them for future use and deployment. Follow these steps to save both the small and large versions to your Google Drive.</p>"},{"location":"base/saving_models/#mounting-google-drive","title":"Mounting Google Drive","text":"<p>First, mount your Google Drive to the Colab environment:</p> <pre><code>from google.colab import drive\nimport os\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n</code></pre> <p>Steps:</p> <ol> <li>Run the above code cell.</li> <li>Follow the prompted link to authorize Colab to access your Google Drive.</li> <li>Paste the authorization code back into the notebook.</li> </ol>"},{"location":"base/saving_models/#saving-the-small-model","title":"Saving the Small Model","text":"<pre><code># Define the directory in Google Drive\nmodel_dir_sm = \"/content/drive/MyDrive/JointNERAnomalyModel\"\nos.makedirs(model_dir_sm, exist_ok=True)\n\n# Save the small model state dictionary\ntorch.save(model_sm.state_dict(), os.path.join(model_dir_sm, \"joint_ner_anomaly_model_sm.pth\"))\nprint(f\"Small model saved to {model_dir_sm}/joint_ner_anomaly_model_sm.pth\")\n</code></pre>"},{"location":"base/saving_models/#saving-the-large-model","title":"Saving the Large Model","text":"<pre><code># Define the directory in Google Drive\nmodel_dir_lg = \"/content/drive/MyDrive/JointNERAnomalyModel\"\nos.makedirs(model_dir_lg, exist_ok=True)\n\n# Save the large model state dictionary\ntorch.save(model_lg.state_dict(), os.path.join(model_dir_lg, \"joint_ner_anomaly_model_lg.pth\"))\nprint(f\"Large model saved to {model_dir_lg}/joint_ner_anomaly_model_lg.pth\")\n</code></pre> <p>\ud83d\udd0d Note:</p> <ul> <li>Ensure that the directory path (<code>/content/drive/MyDrive/JointNERAnomalyModel</code>) exists in your Google Drive. You can change it as per your preference.</li> <li>Keeping models in Google Drive allows easy access and deployment across different environments.</li> </ul> <p>For more details on managing and accessing your saved models, refer to the Inference and Usage section.</p>"},{"location":"base/setup/","title":"Setting Up Google Colab","text":""},{"location":"base/setup/#creating-a-new-notebook","title":"\ud83d\udee0\ufe0f Creating a New Notebook","text":"<p>To begin your experiment, set up a new Google Colab notebook by following these steps:</p> <ul> <li>Access Google Colab:<ul> <li>Open your web browser and navigate to Google Colab.</li> </ul> </li> <li>Create a New Notebook:<ul> <li>Click on <code>File</code> in the top-left corner.</li> <li>Select <code>New Notebook</code> or choose <code>New notebook in Drive</code> to save it directly to your Google Drive.</li> </ul> </li> <li>Name Your Notebook:<ul> <li>Click on the notebook title (default is <code>Untitled</code>) at the top.</li> <li>Rename it appropriately, for example, <code>Joint_NER_Anomaly_Detection-00</code>.</li> </ul> </li> </ul>"},{"location":"base/setup/#configuring-secrets","title":"\ud83d\udd12 Configuring Secrets","text":"<p>To securely access your Hugging Face API key within the notebook, follow these steps:</p> <ol> <li> <p>Open Secrets Manager:</p> <ul> <li>Click on the \"key\" icon on the left sidebar to open the Secrets window.</li> </ul> </li> <li> <p>Add a New Secret:</p> <ul> <li>Click on <code>+ Add a new secret</code>.</li> </ul> </li> <li> <p>Configure the Secret:</p> <ul> <li>Name: <code>HF_API_KEY</code></li> <li>Value: <code>&lt;your_huggingface_api_key&gt;</code> (Replace with your actual Hugging Face API key)</li> </ul> </li> <li> <p>Set Access Permissions:</p> <ul> <li>Toggle <code>Notebook access</code> to <code>Allow</code>.</li> </ul> </li> <li> <p>Save the Secret:</p> <ul> <li>Click <code>Add secret</code> to save.</li> </ul> </li> </ol> <p>\ud83d\udd11 Important: Ensure that your Hugging Face API key is kept confidential and not exposed in the notebook. This key grants access to your Hugging Face account and models.</p> <p>For more details on securely managing secrets, refer to the Saving Models to Google Drive section.</p>"},{"location":"base/troubleshooting/","title":"Troubleshooting","text":""},{"location":"base/troubleshooting/#common-issues-and-solutions","title":"\ud83d\udee0\ufe0f Common Issues and Solutions","text":""},{"location":"base/troubleshooting/#model-loading-errors","title":"\ud83d\udd11 Model Loading Errors","text":"<p>Issue: KeyError 'modernbert'</p> <p>Cause: The <code>BaseModelOutput</code> object returned by ModernBERT does not include a <code>pooler_output</code> attribute, leading to compatibility issues.</p> <p>Solution:</p> <ol> <li>Use CLS Token for Anomaly Classification:</li> <li> <p>Modify the <code>JointNERAnomalyModel</code> to use the <code>[CLS]</code> token embedding from <code>last_hidden_state</code> instead of <code>pooler_output</code>.</p> </li> <li> <p>Updated Forward Method:</p> </li> </ol> <pre><code>def forward(self, input_ids, attention_mask):\n    outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n    sequence_output = outputs.last_hidden_state\n    cls_output = sequence_output[:, 0, :]  # Use CLS token\n\n    ner_logits = self.ner_classifier(sequence_output)\n    anomaly_logits = self.anomaly_classifier(cls_output)\n\n    return ner_logits, anomaly_logits\n</code></pre> <ol> <li>Ensure Transformers Library is Updated:</li> </ol> <pre><code>pip install --upgrade transformers\n</code></pre>"},{"location":"base/troubleshooting/#model-not-found-on-hugging-face","title":"\ud83e\udde9 Model Not Found on Hugging Face","text":"<p>Issue: ValueError: The checkpoint you are trying to load has model type <code>modernbert</code> but Transformers does not recognize this architecture.</p> <p>Cause: The specified ModernBERT model (<code>answerdotai/modernbert-base</code> or <code>answerdotai/modernbert-large</code>) may not exist on Hugging Face's model hub or the Transformers library is outdated.</p> <p>Solution:</p> <ol> <li>Verify Model Availability:</li> <li>Visit the Hugging Face model hub and search for <code>answerdotai/modernbert-base</code> or <code>answerdotai/modernbert-large</code>.</li> <li> <p>If the model does not exist, confirm the correct model name or consult the model provider.</p> </li> <li> <p>Update Transformers Library:</p> </li> </ol> <pre><code>pip install --upgrade transformers\n</code></pre> <ol> <li>Fallback to Compatible Model:</li> <li>If ModernBERT is unavailable, use <code>bert-base-uncased</code> or another compatible model.</li> </ol> <pre><code>model_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModel.from_pretrained(model_name)\n</code></pre>"},{"location":"base/troubleshooting/#cuda-availability-issues","title":"\u26a1 CUDA Availability Issues","text":"<p>Issue: Model runs on CPU instead of GPU.</p> <p>Cause: The Colab runtime is not set to use GPU, or GPU resources are not available.</p> <p>Solution:</p> <ol> <li>Enable GPU in Colab:</li> <li>Click on <code>Runtime</code> in the top menu.</li> <li>Select <code>Change runtime type</code>.</li> <li>In the popup, set <code>Hardware accelerator</code> to <code>GPU</code>.</li> <li> <p>Click <code>Save</code> and restart the runtime if prompted.</p> </li> <li> <p>Verify GPU Availability:</p> </li> </ol> <pre><code>import torch\nprint(torch.cuda.is_available())  # Should return True\n</code></pre>"},{"location":"base/troubleshooting/#insufficient-gpu-memory","title":"\ud83e\udde0 Insufficient GPU Memory","text":"<p>Issue: Out-of-memory (OOM) errors during training.</p> <p>Cause: The model or batch size is too large for the available GPU memory.</p> <p>Solution:</p> <ol> <li>Reduce Batch Size:</li> <li>Decrease the <code>batch_size</code> parameter in your DataLoader.</li> </ol> <pre><code>batch_size = 8  # Example: Reduce from 16 to 8\n</code></pre> <ol> <li>Decrease Maximum Sequence Length:</li> <li>Lower the <code>max_length</code> parameter to reduce memory consumption.</li> </ol> <pre><code>max_length = 128  # Example: Reduce from 256 to 128\n</code></pre> <ol> <li>Use Gradient Accumulation:</li> <li>Accumulate gradients over multiple steps to simulate a larger batch size without increasing memory usage.</li> </ol> <pre><code>accumulation_steps = 4\nfor step, batch in enumerate(train_loader):\n    loss = compute_loss(batch)\n    loss = loss / accumulation_steps\n    loss.backward()\n    if (step + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"base/troubleshooting/#hugging-face-api-key-issues","title":"\ud83d\udee1\ufe0f Hugging Face API Key Issues","text":"<p>Issue: Authentication errors when accessing private models.</p> <p>Cause: Incorrect or missing Hugging Face API key, or insufficient permissions.</p> <p>Solution:</p> <ol> <li>Verify API Key:</li> <li>Ensure that the <code>HF_API_KEY</code> secret is correctly set in the Secrets Manager.</li> <li> <p>Double-check for any typos or missing characters.</p> </li> <li> <p>Check Permissions:</p> </li> <li> <p>Confirm that the API key has the necessary permissions to access the models you intend to use.</p> </li> <li> <p>Reload Secrets:</p> </li> <li>Sometimes, restarting the Colab runtime can help in recognizing newly added secrets.</li> </ol>"},{"location":"base/troubleshooting/#label-mapping-errors","title":"\ud83c\udff7\ufe0f Label Mapping Errors","text":"<p>Issue: KeyError when assigning labels.</p> <p>Cause: Mismatch between entity types in the dataset and those defined in <code>entity_label_map</code>.</p> <p>Solution:</p> <ol> <li>Verify Entity Types:</li> <li> <p>Ensure that all entity types used in your dataset are correctly mapped in <code>entity_label_map</code>.</p> </li> <li> <p>Update <code>entity_label_map</code>:</p> </li> <li>Add any missing entity types to the mapping.</li> </ol> <pre><code>entity_label_map = {\n    \"person\": {\"B\": \"B-PER\", \"I\": \"I-PER\"},\n    \"organization\": {\"B\": \"B-ORG\", \"I\": \"I-ORG\"},\n    \"location\": {\"B\": \"B-LOC\", \"I\": \"I-LOC\"},\n    \"O\": \"O\",\n    \"new_entity_type\": {\"B\": \"B-NEW\", \"I\": \"I-NEW\"}  # Example\n}\n</code></pre> <ol> <li>Ensure Consistent Label IDs:</li> <li>Confirm that <code>label_to_id</code> includes all necessary labels without duplicates.</li> </ol>"},{"location":"base/use_case/","title":"\ud83c\udf1f Use Case: Joint NER and Anomaly Detection","text":"<p>The Joint Named Entity Recognition (NER) and Anomaly Detection model powered by ModernBERT represents a cutting-edge approach to multitask learning. It combines the precision of named entity extraction with the critical functionality of anomaly detection, creating a robust framework for tackling high-stakes challenges across industries.</p>"},{"location":"base/use_case/#why-this-model-is-advanced-novel-and-useful","title":"\ud83c\udfaf Why This Model Is Advanced, Novel, and Useful","text":""},{"location":"base/use_case/#1-dual-purpose-efficiency","title":"1. \ud83d\udd00 Dual-Purpose Efficiency","text":"<p>The model simultaneously performs NER and Anomaly Detection, offering:</p> <ul> <li>Efficient Resource Use: Processes both tasks in a single pass, reducing computational and deployment overhead.</li> <li>Scalable Solutions: Handles large-scale text data in real-time, making it ideal for applications like fraud detection and social media analysis.</li> <li>Unified Outputs: Produces insights that are complementary, such as extracting critical entities while flagging irregular patterns in the same text.</li> </ul>"},{"location":"base/use_case/#2-real-world-relevance","title":"2. \ud83c\udf0d Real-World Relevance","text":"<p>By integrating entity extraction and anomaly detection, the model addresses practical, real-world problems:</p> <ul> <li> <p>Critical Entity Extraction: Identifies structured data such as:</p> <ul> <li>Names (e.g., John Doe)</li> <li>Organizations (e.g., Acme Corp)</li> <li>Locations (e.g., New York)</li> <li>Dates (e.g., January 15, 2025)</li> </ul> </li> <li> <p>Context-Aware Anomaly Detection: Recognizes irregularities, such as:</p> <ul> <li>Unusual transactions in financial logs.</li> <li>Suspicious behavior in chat systems.</li> <li>Fraudulent claims in insurance or healthcare.</li> </ul> </li> </ul> <p>Example:</p> <ul> <li>Input: \"Unauthorized login attempt detected for user admin456 from IP 192.168.1.1.\"</li> <li>Output: <ul> <li>Entities: <code>[(admin456, USERNAME), (192.168.1.1, IP)]</code></li> <li>Anomaly: Yes</li> </ul> </li> </ul> <p>This combination enables actionable insights for organizations handling unstructured data.</p>"},{"location":"base/use_case/#3-leveraging-modernberts-strengths","title":"3. \ud83d\ude80 Leveraging ModernBERT\u2019s Strengths","text":"<p>ModernBERT, with its extended context window and optimized architecture, enhances the model\u2019s performance:</p> <ul> <li>Long-Context Handling: Processes up to 8,192 tokens, making it suitable for long documents like financial reports or legal contracts.</li> <li>Pretrained Power: Adapts to specific domains with minimal fine-tuning.</li> <li>Semantic Awareness: Provides nuanced understanding of relationships within text, ensuring accurate entity extraction and anomaly identification.</li> </ul>"},{"location":"base/use_case/#4-enhanced-anomaly-detection-with-context","title":"4. \ud83e\udd16 Enhanced Anomaly Detection with Context","text":"<p>Traditional anomaly detection models often miss the context. With ModernBERT:</p> <ul> <li>Contextual Anomaly Detection: Identifies irregularities based on the surrounding semantic context.  <ul> <li>For example, a financial transaction flagged as unusual might be tied to specific entities or patterns in text.</li> </ul> </li> <li>Dynamic Pattern Recognition: Learns and adapts to evolving data patterns, crucial for combating fraud or identifying cyber threats.</li> </ul>"},{"location":"base/use_case/#5-a-novel-use-case","title":"5. \u2728 A Novel Use Case","text":"<p>This model is novel because:</p> <ul> <li>No Model Does This: Jointly addressing NER and anomaly detection is novel. It\u2019s an innovative solution to problems where both tasks are critical.</li> <li>Synergistic Insights: The combination of these tasks provides deeper insights, helping organizations not only understand their data but also flag potential risks or irregularities.</li> </ul> <p>Example:</p> <ul> <li>Legal Tech: Extract entities like dates, names, and clauses from contracts while identifying anomalous terms that deviate from standard practices.</li> </ul>"},{"location":"base/use_case/#6-broad-applicability-across-domains","title":"6. \ud83c\udf10 Broad Applicability Across Domains","text":"<p>The joint model is impactful in diverse fields:</p>"},{"location":"base/use_case/#cybersecurity","title":"Cybersecurity","text":"<ul> <li>Example: Flag suspicious login attempts or phishing attempts while extracting usernames and IPs from system logs.</li> <li>Benefit: Automates threat detection and accelerates response times.</li> </ul>"},{"location":"base/use_case/#healthcare","title":"Healthcare","text":"<ul> <li>Example: Analyze patient reports to extract symptoms and treatments while flagging irregularities in treatment plans.</li> <li>Benefit: Enhances patient safety and improves diagnostic accuracy.</li> </ul>"},{"location":"base/use_case/#finance","title":"Finance","text":"<ul> <li>Example: Extract transaction details while flagging anomalies like unusual spending patterns or unauthorized access.</li> <li>Benefit: Automates fraud detection and ensures regulatory compliance.</li> </ul>"},{"location":"base/use_case/#e-commerce","title":"E-Commerce","text":"<ul> <li>Example: Analyze user reviews to extract product mentions while detecting fraudulent or spammy reviews.</li> <li>Benefit: Improves user experience and trust in the platform.</li> </ul>"},{"location":"base/use_case/#legal-tech","title":"Legal Tech","text":"<ul> <li>Example: Extract key entities (e.g., parties, dates, clauses) from contracts while flagging non-standard or risky terms.</li> <li>Benefit: Streamlines contract review and ensures compliance.</li> </ul>"},{"location":"base/use_case/#model-workflow","title":"\ud83d\udee0\ufe0f Model Workflow","text":"<ol> <li>Input Processing: Tokenizes input text using ModernBERT tokenizer.</li> <li>Shared Representation: Leverages ModernBERT's transformer layers for contextual embeddings.</li> <li>NER Output: Token-level predictions for named entity extraction.</li> <li>Anomaly Output: Anomaly classification based on <code>[CLS]</code> token embedding.</li> <li>Unified Results: Outputs entities and anomaly flags for downstream applications.</li> </ol> <p>Refer to the Training the Model and Inference and Usage pages for detailed guidance.</p>"},{"location":"base/use_case/#key-advantages","title":"\u26a1 Key Advantages","text":""},{"location":"base/use_case/#simplified-deployment","title":"Simplified Deployment","text":"<ul> <li>Lower Latency: Combines two tasks into a single model, reducing pipeline complexity.</li> <li>Unified Training: Ensures better task alignment and minimizes biases.</li> </ul>"},{"location":"base/use_case/#scalability","title":"Scalability","text":"<ul> <li>Real-Time Processing: Handles large volumes of text data in mission-critical environments.</li> <li>Domain Adaptability: Easily fine-tuned for specific industries or applications.</li> </ul>"},{"location":"base/use_case/#improved-insights","title":"Improved Insights","text":"<ul> <li>Provides a richer understanding of data by connecting entity extraction with anomaly detection.</li> </ul>"},{"location":"base/use_case/#related-pages","title":"\ud83d\udd17 Related Pages","text":"<ul> <li>Model Versions: Details about small and large model configurations.</li> <li>Training the Model: Step-by-step instructions for training the joint model.</li> <li>Inference and Usage: Guidance on deploying the model for real-world applications.</li> <li>Saving Models: Instructions for saving and managing trained models.</li> <li>Troubleshooting: Solutions for common issues during training or inference.</li> </ul>"},{"location":"base/use_case/#conclusion","title":"\ud83c\udfaf Conclusion","text":"<p>The Joint NER and Anomaly Detection Model powered by ModernBERT combines cutting-edge technology with practical application, making it a transformative tool across industries. Whether you\u2019re tackling fraud detection, cybersecurity, or legal document analysis, this model empowers you to extract structured insights and identify risks with unprecedented efficiency. </p> <p>For step-by-step instructions on setting up and training the model, visit the Setup and Training the Model pages. \ud83d\ude80</p>"},{"location":"embedding_model/","title":"\ud83c\udf1f ModernBERT Embedding Model: Overview \ud83c\udf1f","text":"<p>Welcome to the comprehensive documentation for the ModernBERT Embedding Model! This section is dedicated to exploring the embedding-specific capabilities of ModernBERT, focusing on <code>lightonai/modernbert-embed-large</code> and its related models. These models are optimized to generate high-quality semantic vector embeddings, enabling cutting-edge applications across a variety of industries.</p>"},{"location":"embedding_model/#what-are-embeddings","title":"\ud83e\udde9 What Are Embeddings?","text":"<p>Embeddings are dense, numerical representations of text data that encapsulate its semantic meaning. By transforming words, sentences, or entire documents into high-dimensional vectors, embeddings provide a way to understand and process text computationally. Similar meanings are placed closer together in this vector space, enabling powerful downstream tasks like clustering, search, and similarity detection.</p> <p>Key Characteristics of ModernBERT Embeddings: 1. Semantic Depth: Embeddings reflect deep contextual relationships within and across documents. 2. High Efficiency: Optimized for both speed and scalability using cutting-edge techniques like Flash Attention and Rotary Positional Embeddings (RoPE). 3. Extended Sequence Length: Handles up to 8192 tokens, making it suitable for long-form text embedding.</p> <p> Figure 1: Overview of the ModernBERT Embedding Model workflow, showcasing the transition from input text to embedding vectors and their applications.</p>"},{"location":"embedding_model/#why-modernbert-for-embeddings","title":"\ud83d\ude80 Why ModernBERT for Embeddings?","text":"<p>The ModernBERT Embedding Model offers several advantages over traditional embedding approaches, including: - Bidirectional Context: Encodes information from both past and future tokens for nuanced understanding. - Global and Local Attention: Balances computational efficiency with semantic depth by integrating sliding windows and full sequence attention. - Hardware Awareness: Optimized for consumer GPUs (e.g., NVIDIA RTX series) and high-performance GPUs (e.g., A100, H100). - Fine-Tuning Capability: Adaptable for domain-specific applications with minimal overhead.</p> <p></p> <p>Figure 2: Key applications derived from ModernBERT-generated embedding vectors, with extensibility for custom use cases.</p>"},{"location":"embedding_model/#applications-of-modernbert-embeddings","title":"\ud83c\udfaf Applications of ModernBERT Embeddings","text":"<p>ModernBERT embeddings are versatile and can power applications across industries:</p>"},{"location":"embedding_model/#semantic-search","title":"\ud83e\udde0 Semantic Search","text":"<p>Enable intelligent search systems that retrieve information based on meaning, not just keywords. Applications include: - Knowledge management systems. - Legal document discovery. - E-commerce product search.</p>"},{"location":"embedding_model/#clustering-and-similarity-analysis","title":"\ud83d\udcca Clustering and Similarity Analysis","text":"<p>Group similar texts or measure the semantic similarity between documents, ideal for: - Customer segmentation in marketing. - Topic modeling in academia. - Fraud detection in finance.</p>"},{"location":"embedding_model/#recommendation-systems","title":"\ud83d\udd0d Recommendation Systems","text":"<p>Leverage embeddings to suggest relevant items or content: - Personalized content recommendations in streaming platforms. - Product suggestions in e-commerce. - Learning material recommendations in education.</p>"},{"location":"embedding_model/#domain-specific-use-cases","title":"\ud83c\udfe5 Domain-Specific Use Cases","text":"<p>ModernBERT\u2019s fine-tuning capabilities make it invaluable for industries like: - Healthcare: Clinical text analysis and medical research. - Finance: Fraud detection and sentiment analysis. - Science: Research paper retrieval and clustering.</p>"},{"location":"embedding_model/#unique-features-of-modernbert-embed","title":"\ud83d\udd11 Unique Features of ModernBERT-Embed","text":"<p>Here\u2019s what sets <code>modernbert-embed-large</code> apart:</p> <ol> <li>Extended Token Capacity: Supports embedding texts up to 8192 tokens.</li> <li>Advanced Attention Mechanisms: Alternates local and global attention for computational efficiency.</li> <li>Scalable Performance: Optimized for large-scale tasks with consumer and enterprise hardware.</li> <li>Open-Source Accessibility: Fully available on Hugging Face with an Apache 2.0 license.</li> <li>Hardware-Optimized Design: Takes advantage of GPU-specific features, including Flash Attention.</li> </ol>"},{"location":"embedding_model/#documentation-roadmap","title":"\ud83d\udcda Documentation Roadmap","text":"<p>This folder offers a step-by-step guide for using the ModernBERT Embedding Model effectively. Here\u2019s what you\u2019ll find:</p> <ol> <li>Generating Embeddings: Learn to create embeddings and understand their structure.</li> <li>Semantic Search: Implement semantic search systems with real-world examples.</li> <li>Clustering and Similarity: Explore how to group text and measure similarity.</li> <li>Vector Databases: Integrate embeddings with databases like Pinecone or Milvus.</li> <li>Fine-Tuning: Tailor the embedding model for domain-specific tasks.</li> <li>Performance Benchmarks: Detailed metrics on embedding speed and accuracy.</li> <li>Limitations and Considerations: Key factors to keep in mind while using ModernBERT embeddings.</li> <li>References and Resources: A curated list of references for further reading.</li> </ol>"},{"location":"embedding_model/#figures-cross-referenced-in-this-guide","title":"\ud83d\udcca Figures Cross-Referenced in This Guide","text":"<ul> <li>Figure 1: Embedding Creation Process.</li> <li>Figure 2: Attention Mechanisms in ModernBERT.</li> </ul> <p>For additional details, check:</p> <ul> <li>Clustering and Similarity: Covers Figure 2 in depth.</li> </ul>"},{"location":"embedding_model/#conclusion","title":"\ud83c\udf89 Conclusion","text":"<p>ModernBERT Embedding Model is your gateway to state-of-the-art text understanding and processing. Its advanced features, open-source nature, and scalability make it an essential tool for any NLP application requiring embeddings.</p>"},{"location":"embedding_model/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<p>Dive deeper into specific applications:</p> <ul> <li>Start with Generating Embeddings.</li> <li>Explore Semantic Search to build intelligent retrieval systems.</li> </ul>"},{"location":"embedding_model/#empower-your-nlp-workflows-with-modernbert","title":"\u2728 Empower Your NLP Workflows with ModernBERT! \u2728","text":""},{"location":"embedding_model/use_case/","title":"Use Case","text":"<p>Here's the <code>embedding_model/use_case.md</code> file:</p>"},{"location":"embedding_model/use_case/#modernbert-embedding-model-use-cases","title":"\ud83c\udfaf ModernBERT Embedding Model: Use Cases \ud83c\udfaf","text":"<p>The ModernBERT Embedding Model provides high-quality semantic embeddings, which can be applied across a wide range of industries and NLP tasks. These embeddings capture deep contextual relationships between words, sentences, and documents, enabling applications in search, recommendation systems, text clustering, and more. In this section, we explore some of the most compelling use cases of the ModernBERT Embedding Model.</p>"},{"location":"embedding_model/use_case/#use-case-1-semantic-search","title":"\ud83d\udcda Use Case 1: Semantic Search","text":""},{"location":"embedding_model/use_case/#overview","title":"\ud83e\udde0 Overview","text":"<p>Semantic search uses embeddings to retrieve information based on the meaning behind the search query, rather than relying solely on keyword matching. By understanding the intent and context of the search, semantic search systems can return more relevant and accurate results.</p> <p>Benefits: - Improved Accuracy: Returns results based on contextual meaning. - Handles Synonyms: Recognizes similar words with the same meaning. - Context-Aware: Understands the intent behind ambiguous queries.</p>"},{"location":"embedding_model/use_case/#implementation","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation","text":"<p>Using ModernBERT embeddings, you can create a semantic search engine that transforms both the query and documents into embeddings. These embeddings are then compared to find the closest matches, using techniques like cosine similarity or Euclidean distance.</p> <p>Example Workflow: 1. Step 1: Convert the search query into an embedding. 2. Step 2: Convert documents or text chunks into embeddings. 3. Step 3: Measure the cosine similarity between the query and each document. 4. Step 4: Rank and return the most relevant documents.</p> <p>Figure 1: Semantic Search Process (cross-referenced in Semantic Search).</p> <pre><code>flowchart LR\n    A[Search Query] --&gt; B[ModernBERT Embedding]\n    C[Document Embeddings] --&gt; D[Similarity Calculation]\n    D --&gt; E[Top Matches]\n</code></pre>"},{"location":"embedding_model/use_case/#applications-in-semantic-search","title":"\ud83d\udca1 Applications in Semantic Search:","text":"<ul> <li>Enterprise Knowledge Management: Easily search across a vast corpus of documents, finding the most relevant information based on intent.</li> <li>Legal Text Search: Quickly retrieve relevant legal documents, case studies, or contracts based on context.</li> <li>Customer Support: Enable more accurate and efficient searches in FAQ systems or knowledge bases.</li> </ul>"},{"location":"embedding_model/use_case/#use-case-2-clustering-and-categorization","title":"\ud83d\udee0\ufe0f Use Case 2: Clustering and Categorization","text":""},{"location":"embedding_model/use_case/#overview_1","title":"\ud83e\udde0 Overview","text":"<p>Clustering involves grouping similar pieces of text together based on their semantic meaning. Using ModernBERT embeddings, texts with similar themes, topics, or content can be automatically clustered, making it easier to analyze large datasets.</p> <p>Benefits: - Automated Organization: Grouping similar content helps organize large datasets automatically. - Topic Discovery: Identify hidden themes and trends in data. - Scalable: Can be applied to a vast amount of unstructured text.</p>"},{"location":"embedding_model/use_case/#implementation_1","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation","text":"<p>ModernBERT embeddings can be used as the basis for clustering algorithms like K-means or hierarchical clustering. By representing text as dense vectors, the clustering algorithm groups texts with similar embeddings, making it possible to categorize large corpora of documents effectively.</p> <p>Example Workflow: 1. Step 1: Convert texts into ModernBERT embeddings. 2. Step 2: Apply a clustering algorithm like K-means or DBSCAN. 3. Step 3: Analyze the clusters for meaningful categories or insights.</p> <p>Figure 2: Clustering Texts Based on Embeddings (cross-referenced in Clustering and Similarity).</p> <pre><code>pie\n    title Text Cluster Distribution\n    \"Topic A\" : 40\n    \"Topic B\" : 35\n    \"Topic C\" : 25\n</code></pre>"},{"location":"embedding_model/use_case/#applications-in-clustering","title":"\ud83d\udca1 Applications in Clustering:","text":"<ul> <li>Customer Sentiment Analysis: Group customer feedback based on sentiment or intent.</li> <li>News Article Categorization: Automatically categorize articles by topics like politics, health, or sports.</li> <li>Content Tagging: Automatically tag content with relevant labels based on its semantic content.</li> </ul>"},{"location":"embedding_model/use_case/#use-case-3-personalized-recommendation-systems","title":"\ud83d\uded2 Use Case 3: Personalized Recommendation Systems","text":""},{"location":"embedding_model/use_case/#overview_2","title":"\ud83e\udde0 Overview","text":"<p>ModernBERT embeddings can power recommendation systems by comparing the embeddings of user profiles and items (products, movies, articles) to recommend content that is semantically similar to what a user has interacted with or shown interest in.</p> <p>Benefits: - Context-Aware: Recommendations are based on deeper understanding rather than surface-level patterns. - Enhanced Accuracy: Recommends more relevant content by considering the user's specific preferences. - Personalized Experience: Offers a tailored experience for each user.</p>"},{"location":"embedding_model/use_case/#implementation_2","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation","text":"<p>The recommendation engine works by generating embeddings for both user profiles and items (e.g., products, movies). The system then computes the cosine similarity between the user\u2019s embedding and the embeddings of items in the catalog. The items with the highest similarity scores are presented as recommendations.</p> <p>Example Workflow: 1. Step 1: Generate embeddings for the user\u2019s preferences and past interactions. 2. Step 2: Create embeddings for items in the catalog. 3. Step 3: Calculate the similarity between the user embedding and item embeddings. 4. Step 4: Rank the items by similarity and recommend the most relevant ones.</p> <p>Figure 3: Personalized Recommendation Workflow (cross-referenced in Fine-tuning for Custom Embeddings).</p> <pre><code>sequenceDiagram\n    participant User\n    participant Item1\n    participant Item2\n    participant EmbeddingModel\n    User-&gt;&gt;EmbeddingModel: Profile Embedding\n    Item1-&gt;&gt;EmbeddingModel: Item Embedding\n    Item2-&gt;&gt;EmbeddingModel: Item Embedding\n    EmbeddingModel-&gt;&gt;User: Rank Items by Similarity\n    User-&gt;&gt;User: Recommend Top Item\n</code></pre>"},{"location":"embedding_model/use_case/#applications-in-recommendation-systems","title":"\ud83d\udca1 Applications in Recommendation Systems:","text":"<ul> <li>E-commerce: Recommend products to users based on previous purchases or browsing history.</li> <li>Content Streaming: Suggest movies, shows, or songs based on viewing or listening habits.</li> <li>Social Media: Recommend posts, pages, or groups based on users\u2019 past interactions and content preferences.</li> </ul>"},{"location":"embedding_model/use_case/#use-case-4-healthcare-text-analysis","title":"\ud83c\udfe5 Use Case 4: Healthcare Text Analysis","text":""},{"location":"embedding_model/use_case/#overview_3","title":"\ud83e\udde0 Overview","text":"<p>In healthcare, analyzing medical texts, clinical notes, and research papers can uncover valuable insights. ModernBERT embeddings can be used to improve medical document retrieval, patient record categorization, and even automated diagnosis suggestions.</p> <p>Benefits: - Semantic Understanding: Deep contextual understanding of medical terminology and relationships. - Improved Diagnosis: Assist in detecting potential conditions from clinical notes. - Enhanced Patient Record Categorization: Automate the classification and tagging of medical records.</p>"},{"location":"embedding_model/use_case/#implementation_3","title":"\ud83e\uddd1\u200d\ud83d\udcbb Implementation","text":"<p>Medical documents, including clinical notes, research papers, and medical records, are converted into ModernBERT embeddings. These embeddings are used for tasks like document classification, medical question answering, and content recommendation.</p> <p>Example Workflow: 1. Step 1: Convert medical texts into embeddings. 2. Step 2: Use embeddings for classification or clustering. 3. Step 3: Integrate the embeddings into a system for real-time clinical decision support.</p> <p>Figure 4: Healthcare Text Analysis Workflow (cross-referenced in Applications).</p> <pre><code>graph LR\n    A[Medical Records] --&gt; B[ModernBERT Embedding]\n    B --&gt; C[Patient Categorization]\n    C --&gt; D[Decision Support System]\n    D --&gt; E[Diagnosis Suggestions]\n</code></pre>"},{"location":"embedding_model/use_case/#applications-in-healthcare","title":"\ud83d\udca1 Applications in Healthcare:","text":"<ul> <li>Clinical Decision Support: Assist medical professionals by providing relevant clinical data and diagnosis suggestions.</li> <li>Medical Research: Automatically group research papers by topic and suggest similar studies.</li> <li>Patient Record Management: Categorize and tag patient records automatically for easier retrieval and analysis.</li> </ul>"},{"location":"embedding_model/use_case/#conclusion","title":"\ud83d\udcdd Conclusion","text":"<p>The ModernBERT Embedding Model is a versatile tool that powers a wide range of applications, from semantic search to personalized recommendations, clustering, and even specialized use cases like healthcare text analysis. Its powerful features make it suitable for a variety of industries and use cases, allowing for deeper insights and more efficient workflows.</p>"},{"location":"embedding_model/use_case/#start-exploring-modernbert-embedding-today","title":"\ud83c\udf1f Start Exploring ModernBERT Embedding Today! \ud83c\udf1f","text":""},{"location":"overview/applications/","title":"Applications of ModernBERT \ud83c\udf1f","text":"<p>ModernBERT\u2019s advanced architecture, extended context window, and exceptional efficiency make it a versatile powerhouse in various industries. From legal analysis to conversational AI, ModernBERT is designed to handle complex tasks with precision and speed. Below are some of its most impactful applications.</p>"},{"location":"overview/applications/#legal-and-compliance","title":"\ud83d\udcdc Legal and Compliance","text":"<p>Legal and compliance tasks often require the processing of lengthy documents, complex language, and domain-specific terminology. ModernBERT excels in these scenarios:</p> <ul> <li>Contract Analysis: Automatically extract clauses, obligations, and penalties from contracts.</li> <li>Regulatory Compliance: Identify and flag non-compliant sections in legal documents based on regulatory guidelines.</li> <li>Case Law Summarization: Summarize extensive case laws or court judgments for quick review.</li> </ul>"},{"location":"overview/applications/#example-use-case","title":"Example Use Case","text":"<p>ModernBERT can process a 200-page legal document in one go, extracting key obligations and delivering a summarized overview to legal professionals.</p>"},{"location":"overview/applications/#healthcare-and-medicine","title":"\ud83c\udfe5 Healthcare and Medicine","text":"<p>The healthcare sector generates vast amounts of unstructured data that require processing and interpretation. ModernBERT is an ideal tool for:</p> <ul> <li>Medical Record Analysis: Summarize patient histories for efficient review by healthcare providers.</li> <li>Clinical Research: Extract relevant information from clinical trial documents and research papers.</li> <li>Symptom Analysis: Power intelligent symptom-checking tools for digital healthcare assistants.</li> </ul>"},{"location":"overview/applications/#example-use-case_1","title":"Example Use Case","text":"<p>In electronic health records (EHR) systems, ModernBERT can identify critical information such as allergies or past treatments and summarize it for physicians.</p>"},{"location":"overview/applications/#conversational-ai","title":"\ud83d\udcac Conversational AI","text":"<p>ModernBERT\u2019s ability to handle extended contexts and its nuanced understanding of language make it a game-changer for conversational systems:</p> <ul> <li>Chatbots: Provide context-aware and coherent responses in multi-turn dialogues.</li> <li>Customer Support: Enable dynamic and personalized customer service across industries.</li> <li>Sentiment Analysis: Monitor user sentiment in real-time to adjust the conversation tone or escalate issues.</li> </ul>"},{"location":"overview/applications/#example-use-case_2","title":"Example Use Case","text":"<p>A chatbot powered by ModernBERT can track a conversation over 20 turns, retaining and referencing previous inputs for continuity and user satisfaction.</p>"},{"location":"overview/applications/#finance-and-business-intelligence","title":"\ud83d\udcca Finance and Business Intelligence","text":"<p>The financial sector requires precise analysis of structured and unstructured data. ModernBERT supports:</p> <ul> <li>Market Sentiment Analysis: Analyze news articles, financial reports, and social media for market trends.</li> <li>Earnings Report Summarization: Quickly summarize quarterly earnings reports for investors.</li> <li>Fraud Detection: Identify anomalies or suspicious patterns in transaction records.</li> </ul>"},{"location":"overview/applications/#example-use-case_3","title":"Example Use Case","text":"<p>ModernBERT can process years of financial filings and highlight inconsistencies or critical points for analysts.</p>"},{"location":"overview/applications/#education-and-research","title":"\ud83d\udcda Education and Research","text":"<p>ModernBERT is an invaluable tool for academics and researchers working with large corpora of data:</p> <ul> <li>Academic Research: Extract insights and citations from a collection of research papers.</li> <li>Language Tutoring: Power language-learning apps with grammar checks and contextual suggestions.</li> <li>Knowledge Management: Summarize key findings from multiple sources for literature reviews.</li> </ul>"},{"location":"overview/applications/#example-use-case_4","title":"Example Use Case","text":"<p>A research assistant application powered by ModernBERT can review hundreds of papers and provide a summary of relevant findings for a specific topic.</p>"},{"location":"overview/applications/#e-commerce","title":"\ud83d\uded2 E-Commerce","text":"<p>E-commerce platforms generate vast amounts of data from user interactions. ModernBERT can enhance:</p> <ul> <li>Personalized Recommendations: Analyze user reviews and browsing history to deliver tailored product suggestions.</li> <li>Review Analysis: Summarize customer feedback to identify recurring issues or popular features.</li> <li>Search Optimization: Improve search relevance by understanding user intent and context.</li> </ul>"},{"location":"overview/applications/#example-use-case_5","title":"Example Use Case","text":"<p>ModernBERT can analyze product reviews and generate concise summaries, helping users make informed decisions quickly.</p>"},{"location":"overview/applications/#creative-applications","title":"\ud83c\udfa8 Creative Applications","text":"<p>ModernBERT isn\u2019t limited to structured or professional domains\u2014it also empowers creative applications:</p> <ul> <li>Content Creation: Assist writers with brainstorming ideas, generating content, or editing drafts.</li> <li>Scriptwriting: Generate coherent dialogues or narrative arcs for stories.</li> <li>Poetry and Lyrics: Produce creative text in the form of poems or song lyrics based on prompts.</li> </ul>"},{"location":"overview/applications/#example-use-case_6","title":"Example Use Case","text":"<p>Using ModernBERT, a writer can input a few lines of a story and receive multiple suggestions for continuing the narrative.</p>"},{"location":"overview/applications/#scientific-and-technical-domains","title":"\ud83c\udfd7\ufe0f Scientific and Technical Domains","text":"<p>ModernBERT is well-suited for highly specialized and technical fields:</p> <ul> <li>Engineering Documentation: Summarize or generate technical manuals and specifications.</li> <li>Scientific Data Analysis: Extract and organize data from complex scientific reports.</li> <li>Patent Review: Analyze patents for originality and overlapping claims.</li> </ul>"},{"location":"overview/applications/#example-use-case_7","title":"Example Use Case","text":"<p>ModernBERT can parse technical specifications for a new product and identify missing details or ambiguities.</p>"},{"location":"overview/applications/#multilingual-applications","title":"\ud83c\udf10 Multilingual Applications","text":"<p>ModernBERT\u2019s architecture can be extended to support multilingual tasks, enabling it to:</p> <ul> <li>Translate Contextually: Provide translations that consider the full document context.</li> <li>Cross-Language Information Retrieval: Search and summarize information in one language from sources in another.</li> <li>Multilingual Chatbots: Support conversations in multiple languages with accurate context retention.</li> </ul>"},{"location":"overview/applications/#conclusion","title":"\ud83c\udfaf Conclusion \ud83d\ude80","text":"<p>ModernBERT\u2019s versatility ensures it can be applied across industries and use cases, offering advanced capabilities for long-context understanding and precise language processing. Whether it\u2019s legal analysis, customer support, or creative writing, ModernBERT provides the tools to transform complex tasks into actionable insights.</p>"},{"location":"overview/architecture/","title":"ModernBERT Architecture \ud83c\udfd7\ufe0f","text":"<p>ModernBERT builds upon the robust foundation of the original BERT architecture while introducing a host of advancements to enhance scalability, efficiency, and performance. In this section, we take a deep dive into its architectural innovations and what makes it a cutting-edge model.</p>"},{"location":"overview/architecture/#core-building-blocks","title":"\ud83e\uddf1 Core Building Blocks","text":"<p>At its heart, ModernBERT retains the fundamental components of a transformer-based encoder model, including:</p> <ol> <li>Multi-Head Self-Attention: Allows the model to weigh relationships between tokens bidirectionally.</li> <li>Feedforward Neural Networks (FFNNs): Positioned between attention layers for feature transformation.</li> <li>Positional Encoding: Provides a mechanism for sequence order awareness.</li> </ol> <p>While these foundational elements remain similar to the original BERT, ModernBERT introduces several key improvements.</p>"},{"location":"overview/architecture/#dynamic-masking","title":"\ud83d\udd04 Dynamic Masking","text":"<p>Traditional BERT uses static masking, where the same tokens are masked during every training epoch. ModernBERT replaces this with dynamic masking, ensuring:</p> <ul> <li>Greater diversity in masked token patterns.</li> <li>Enhanced robustness to unseen data.</li> <li>Better generalization during fine-tuning.</li> </ul>"},{"location":"overview/architecture/#attention-mechanism-innovations","title":"\ud83d\udd0d Attention Mechanism Innovations","text":"<p>ModernBERT employs advanced attention mechanisms to handle longer sequences efficiently:</p>"},{"location":"overview/architecture/#1-sparse-attention","title":"1. Sparse Attention","text":"<ul> <li>Instead of attending to all tokens, the model selectively focuses on the most relevant ones.</li> <li>Reduces computational complexity from quadratic to linear for certain tasks.</li> <li>Enables processing of longer contexts without exponentially increasing memory requirements.</li> </ul>"},{"location":"overview/architecture/#2-sliding-window-attention","title":"2. Sliding Window Attention","text":"<ul> <li>Divides long input sequences into overlapping chunks.</li> <li>Ensures that information from one window flows into the next, preserving global context while reducing memory usage.</li> </ul>"},{"location":"overview/architecture/#3-global-local-attention","title":"3. Global-Local Attention","text":"<ul> <li>Combines global tokens that represent overarching context with local tokens for detailed focus.</li> <li>Balances between long-range dependencies and localized patterns.</li> </ul>"},{"location":"overview/architecture/#layer-normalization-enhancements","title":"\ud83d\udee0\ufe0f Layer Normalization Enhancements","text":"<p>ModernBERT improves the stability and efficiency of training by introducing Pre-Norm Layer Normalization, where normalization is applied before the attention and feedforward layers. This adjustment:</p> <ul> <li>Improves gradient flow in deep models.</li> <li>Reduces convergence time during training.</li> <li>Enhances scalability, enabling deeper architectures.</li> </ul>"},{"location":"overview/architecture/#optimized-memory-and-speed","title":"\ud83c\udfce\ufe0f Optimized Memory and Speed","text":""},{"location":"overview/architecture/#1-key-value-caching","title":"1. Key-Value Caching","text":"<p>During inference, ModernBERT uses key-value caching to store intermediate results from the attention mechanism, allowing faster token-by-token processing for tasks like autoregressive generation.</p>"},{"location":"overview/architecture/#2-mixed-precision-support","title":"2. Mixed Precision Support","text":"<ul> <li>Full integration of FP16 for reduced memory footprint.</li> <li>Enables deployment on consumer-grade GPUs without compromising accuracy.</li> </ul>"},{"location":"overview/architecture/#parameter-configurations","title":"\ud83d\udd22 Parameter Configurations","text":"<p>ModernBERT is available in two main configurations, designed to cater to different resource and performance needs:</p> <ol> <li> <p>Base Model:</p> <ul> <li>~149 million parameters.</li> <li>Suitable for standard tasks with limited computational resources.</li> </ul> </li> <li> <p>Large Model:</p> <ul> <li>~395 million parameters.</li> <li>Ideal for complex tasks requiring greater depth and nuance.</li> </ul> </li> </ol> <p>Both configurations can process up to 8,192 tokens, making them highly versatile for long-context scenarios.</p>"},{"location":"overview/architecture/#training-innovations","title":"\ud83e\uddea Training Innovations","text":"<p>ModernBERT was pretrained on 2 trillion tokens using cutting-edge techniques:</p> <ul> <li>RoPE (Rotary Positional Embeddings): Replaces traditional positional encodings to better handle long sequences.</li> <li>Efficient Token Shuffling: Introduces randomness in training batches to prevent overfitting.</li> <li>Sentence Ordering Tasks: Enhances the model's ability to understand sequence relationships, improving coherence in applications like summarization and multi-turn dialogue.</li> </ul>"},{"location":"overview/architecture/#versatility-and-real-world-adaptability","title":"\ud83c\udf10 Versatility and Real-World Adaptability","text":"<p>ModernBERT\u2019s architecture is designed with practical use cases in mind:</p> <ul> <li>Drop-in Replacement: Fully compatible with existing BERT pipelines and fine-tuning setups.</li> <li>Plug-and-Play Efficiency: Optimized for inference on GPUs ranging from consumer-grade (RTX 2060) to enterprise-level accelerators (A100, V100).</li> <li>Scalability: Can be scaled for distributed training or deployed in lightweight environments using quantization.</li> </ul>"},{"location":"overview/architecture/#closing-thoughts","title":"\ud83d\ude80 Closing Thoughts","text":"<p>ModernBERT\u2019s architecture exemplifies the future of NLP, where models are not just larger but smarter and more efficient. With its innovative attention mechanisms, training optimizations, and long-context capabilities, ModernBERT is a versatile powerhouse for a wide range of applications.</p>"},{"location":"overview/features/","title":"Key Features of ModernBERT \ud83d\ude80","text":"<p>ModernBERT represents a leap forward in transformer-based architectures, addressing critical limitations of earlier models while introducing innovative features for cutting-edge NLP applications. Below, we explore its standout capabilities and what makes ModernBERT a game-changer.</p>"},{"location":"overview/features/#1-extended-context-window","title":"\ud83d\udcdc 1. Extended Context Window","text":"<p>ModernBERT supports sequences of up to 8,192 tokens, dwarfing the 512-token limit of the original BERT model. This feature enables the model to process and understand:</p> <ul> <li>Long-form documents: Analyze entire contracts, research papers, or books without fragmenting them into smaller chunks.</li> <li>Complex dialogues: Retain context over multi-turn conversations in chatbots or customer service systems.</li> <li>Hierarchical relationships: Process nested structures in large datasets, such as legal hierarchies or XML documents.</li> </ul> <p>By eliminating the need for document chunking, ModernBERT reduces the risks of losing important context while enhancing accuracy and interpretability.</p>"},{"location":"overview/features/#2-optimized-performance","title":"\ud83c\udfce\ufe0f 2. Optimized Performance","text":"<p>Despite its larger context window and expanded feature set, ModernBERT is highly optimized for speed and efficiency:</p> <ul> <li>Fast inference: Tailored for deployment on commonly used GPUs, including mid-range hardware like the RTX 2060.</li> <li>Reduced memory footprint: Employs memory-efficient techniques like sparse attention and layer caching, allowing long-sequence processing without linear memory growth.</li> <li>FP16 support: Mixed-precision inference reduces computational demands while maintaining high accuracy.</li> </ul> <p>These optimizations ensure that ModernBERT delivers top-tier performance without demanding specialized hardware.</p>"},{"location":"overview/features/#3-robust-training-data","title":"\ud83d\udd0d 3. Robust Training Data","text":"<p>ModernBERT was pretrained on an unprecedented corpus of 2 trillion tokens, incorporating a diverse range of data sources, including:</p> <ul> <li>News articles, scientific publications, and encyclopedias.</li> <li>User-generated content like blogs, social media, and reviews.</li> <li>Domain-specific texts for finance, healthcare, and law.</li> </ul> <p>This comprehensive dataset ensures that ModernBERT excels across various domains, demonstrating unparalleled generalization and domain adaptation.</p>"},{"location":"overview/features/#4-architectural-innovations","title":"\ud83d\udee0\ufe0f 4. Architectural Innovations","text":"<p>ModernBERT incorporates multiple architectural upgrades over its predecessor:</p> <ol> <li> <p>Dynamic Masking:</p> <ul> <li>Masks are generated dynamically during training to ensure diverse predictions.</li> <li>This improves the model's robustness and adaptability to unseen data.</li> </ul> </li> <li> <p>Attention Mechanisms:</p> <ul> <li>Introduces sparse attention to focus on relevant parts of long sequences efficiently.</li> <li>Implements global-local attention patterns, balancing the need for global understanding with localized context.</li> </ul> </li> <li> <p>Layer Normalization Enhancements:</p> <ul> <li>Improved convergence stability during training.</li> <li>Allows deeper architectures to scale without diminishing returns.</li> </ul> </li> </ol>"},{"location":"overview/features/#5-state-of-the-art-performance","title":"\ud83d\udcca 5. State-of-the-Art Performance","text":"<p>ModernBERT consistently achieves state-of-the-art results across industry-standard benchmarks, including:</p> <ul> <li>GLUE and SuperGLUE: Excelling in tasks like sentiment analysis, language inference, and reasoning.</li> <li>MS MARCO and BEIR: Demonstrating superior performance in retrieval-based applications, including search engines and recommender systems.</li> <li>Domain-Specific Tasks: Outperforming competitors in specialized fields like legal tech, financial analysis, and healthcare.</li> </ul>"},{"location":"overview/features/#6-real-world-applications","title":"\ud83c\udf0d 6. Real-World Applications","text":"<p>The versatility of ModernBERT makes it ideal for a broad spectrum of use cases:</p> <ul> <li>Legal and Compliance: Automatically parse and extract clauses from lengthy contracts.</li> <li>Healthcare: Summarize patient medical histories and recommend treatments.</li> <li>Customer Service: Power multi-turn chatbots with long-term memory and improved conversational flow.</li> <li>Finance: Analyze trends in financial documents, earnings reports, and market summaries.</li> </ul>"},{"location":"overview/features/#7-seamless-compatibility","title":"\ud83c\udfaf 7. Seamless Compatibility","text":"<p>ModernBERT is designed to integrate smoothly into existing NLP workflows:</p> <ul> <li>Fully compatible with Hugging Face Transformers, allowing easy access to tokenizer and model utilities.</li> <li>Available in two configurations:<ul> <li>Base Model (149M Parameters): For tasks with constrained resources.</li> <li>Large Model (395M Parameters): For tasks requiring greater depth and nuance.</li> </ul> </li> </ul>"},{"location":"overview/features/#closing-thoughts","title":"\ud83c\udfc1 Closing Thoughts \ud83d\udca1","text":"<p>ModernBERT's enhanced context window, optimized performance, and architectural innovations set a new standard for transformer-based models. Whether you're building a chatbot, analyzing lengthy documents, or exploring cutting-edge NLP applications, ModernBERT provides the tools to succeed.</p>"},{"location":"overview/introduction/","title":"ModernBERT Overview","text":""},{"location":"overview/introduction/#introduction-to-modernbert","title":"Introduction to ModernBERT","text":"<p>Since its inception in 2018, BERT (Bidirectional Encoder Representations from Transformers) revolutionized the field of Natural Language Processing (NLP). By leveraging the concept of bidirectional attention, it introduced a significant leap in understanding language context, enabling breakthroughs in diverse tasks like question answering, text classification, and named entity recognition. However, as NLP use cases grow more complex, challenges such as limited context length, computational inefficiency, and model scalability have emerged. </p> <p>ModernBERT steps in as the evolution of BERT, addressing these limitations while building on the robust foundation of transformer architectures. It has been developed to meet the rising demands for models that are:</p> <ul> <li>Efficient in real-world scenarios.</li> <li>Capable of processing longer contexts without sacrificing performance.</li> <li>Designed to integrate seamlessly into existing workflows.</li> </ul> <p>ModernBERT offers advanced capabilities and introduces optimizations that make it a powerful replacement for its predecessors in tasks requiring speed, accuracy, and scalability.</p>"},{"location":"overview/introduction/#key-features-of-modernbert","title":"Key Features of ModernBERT","text":""},{"location":"overview/introduction/#1-extended-context-window","title":"1. Extended Context Window","text":"<p>One of the most transformative features of ModernBERT is its extended context window, enabling the model to process sequences with up to 8,192 tokens natively. </p> <p>This is a monumental improvement over traditional BERT models, which were typically constrained to 512 tokens. The ability to comprehend and process longer sequences is essential for tasks involving:</p> <ul> <li>Long-form documents such as legal contracts, scientific papers, and books.</li> <li>Dialogue systems requiring retention of multi-turn conversational context.</li> <li>Multi-vector retrieval where large spans of text are analyzed.</li> </ul> <p>This extended window minimizes the need for workarounds like document chunking, reducing the risk of losing contextual integrity.</p>"},{"location":"overview/introduction/#2-enhanced-training-and-pretraining","title":"2. Enhanced Training and Pretraining","text":"<p>ModernBERT was trained on a dataset exceeding 2 trillion tokens, significantly larger than the pretraining corpora used for BERT and its variants. This extensive training allows ModernBERT to:</p> <ul> <li>Capture broader language patterns and nuances across diverse domains.</li> <li>Excel in low-resource scenarios by generalizing effectively to unseen tasks.</li> </ul> <p>The model uses dynamic masking techniques, enhancing its ability to predict relationships and dependencies in sequences during pretraining.</p>"},{"location":"overview/introduction/#3-architectural-improvements","title":"3. Architectural Improvements","text":"<p>ModernBERT introduces multiple architectural enhancements designed for efficiency and scalability:</p> <ul> <li>Improved Layer Normalization: This speeds up convergence during training while improving stability.</li> <li>Attention Optimizations: Techniques like sparse attention and efficient key-value caching are incorporated, allowing the model to handle long-range dependencies without linear scaling of memory.</li> <li>Reduced Latency: Optimized for GPUs and modern hardware, ModernBERT achieves faster inference speeds compared to other BERT-like models.</li> </ul>"},{"location":"overview/introduction/#4-efficient-deployment-and-compatibility","title":"4. Efficient Deployment and Compatibility","text":"<p>ModernBERT is designed with real-world usability in mind. It supports two main configurations tailored to different resource constraints:</p> <ul> <li>Base Model (149M Parameters): Ideal for standard NLP tasks where efficiency is critical.</li> <li>Large Model (395M Parameters): Suited for applications requiring deeper language understanding, such as high-accuracy classification or advanced entity recognition.</li> </ul> <p>Both variants are compatible with existing Hugging Face libraries and can be deployed using widely available GPUs like the RTX 2060. Techniques such as mixed-precision inference (FP16) further enhance deployment flexibility.</p>"},{"location":"overview/introduction/#5-performance-benchmarks","title":"5. Performance Benchmarks","text":"<p>ModernBERT demonstrates state-of-the-art results across multiple NLP benchmarks:</p> <ul> <li>GLUE Tasks: Outperforms older models on classification tasks, including MNLI, QNLI, and SST-2.</li> <li>SuperGLUE: Achieves competitive results on complex reasoning and language inference benchmarks.</li> <li>Multi-vector Retrieval: Excels in retrieval-based tasks where documents are ranked based on relevance.</li> </ul>"},{"location":"overview/introduction/#deep-dive-into-modernbert","title":"Deep Dive into ModernBERT","text":""},{"location":"overview/introduction/#pretraining-objectives","title":"Pretraining Objectives","text":"<p>ModernBERT retains the core masked language modeling (MLM) objective from BERT but introduces several enhancements:</p> <ul> <li>Dynamic Masking: Masked tokens are adjusted during training epochs to improve diversity in predictions.</li> <li>Sentence Ordering Tasks: Instead of Next Sentence Prediction (NSP), ModernBERT uses more robust techniques for predicting sentence relationships, leading to improved coherence in tasks like summarization.</li> </ul>"},{"location":"overview/introduction/#handling-long-contexts","title":"Handling Long Contexts","text":"<p>The extended context capabilities of ModernBERT are achieved through efficient attention mechanisms:</p> <ol> <li>Sparse Attention: Reduces the computational cost of attending to every token, enabling the model to focus on relevant segments of longer sequences.</li> <li>Sliding Window Attention: Processes overlapping chunks to maintain context continuity without memory explosion.</li> </ol> <p>These methods ensure that ModernBERT can process long documents while maintaining low latency and high accuracy.</p>"},{"location":"overview/introduction/#practical-applications","title":"Practical Applications","text":"<p>ModernBERT\u2019s versatility makes it suitable for a wide range of NLP applications:</p> <ul> <li>Legal Tech: Parsing lengthy contracts and extracting clauses.</li> <li>Healthcare: Analyzing patient records for insights.</li> <li>Finance: Evaluating annual reports and financial documents for sentiment and trends.</li> <li>Customer Support: Powering conversational AI systems with long-term memory.</li> </ul>"},{"location":"overview/introduction/#conclusion","title":"Conclusion","text":"<p>ModernBERT represents a paradigm shift in the evolution of transformer-based models. By addressing the limitations of traditional BERT, it emerges as a high-performance, scalable, and efficient solution for modern NLP challenges. Its extended context window, architectural innovations, and real-world optimizations make it an invaluable tool for researchers and developers alike.</p> <p>As the NLP landscape continues to evolve, ModernBERT stands as a testament to how transformer models can adapt to growing demands, paving the way for even more sophisticated applications.</p>"},{"location":"overview/learn_more/","title":"Learn More About ModernBERT \ud83d\udcda","text":"<p>ModernBERT is a cutting-edge encoder-only Transformer model with advanced capabilities for natural language processing (NLP). Explore these curated resources, including blogs, videos, and official repositories, to dive deeper into its architecture, features, and applications.</p>"},{"location":"overview/learn_more/#hugging-face-resources","title":"\ud83c\udf10 Hugging Face Resources","text":""},{"location":"overview/learn_more/#1-hugging-face-blog","title":"1. Hugging Face Blog","text":"<p>Finally, a Replacement for BERT: Introducing ModernBERT An in-depth blog detailing ModernBERT\u2019s advancements over traditional BERT models. Learn about its extended 8,192 token sequence length, faster processing, and improved performance across a variety of NLP benchmarks.</p>"},{"location":"overview/learn_more/#2-modernbert-model-repository","title":"2. ModernBERT Model Repository","text":"<p>ModernBERT-base on Hugging Face The official Hugging Face model card for ModernBERT. Includes: - Pretrained weights and tokenizer. - Instructions for deployment and fine-tuning. - Performance benchmarks and intended use cases.</p>"},{"location":"overview/learn_more/#youtube-videos","title":"\ud83c\udfa5 YouTube Videos","text":""},{"location":"overview/learn_more/#1-new-transformer-for-rag-modernbert","title":"1. NEW Transformer for RAG: ModernBERT","text":"<p>This video introduces ModernBERT, a groundbreaking open-source encoder-only Transformer model, highlighting its architectural advancements, computational efficiency, and transformative potential in applications like information retrieval, recommendation systems, and retrieval-augmented generation (RAG) pipelines.</p>"},{"location":"overview/learn_more/#2-modernbert-a-highly-efficient-encoder-only-transformer-model","title":"2. ModernBERT: A Highly Efficient Encoder-Only Transformer Model","text":"<p>A detailed video highlighting ModernBERT's ability to process long sequences efficiently while maintaining state-of-the-art accuracy in language understanding tasks.</p>"},{"location":"overview/learn_more/#3-modernbert-the-next-generation-of-language-encoders","title":"3. ModernBERT: The Next Generation of Language Encoders","text":"<p>A technical deep dive into ModernBERT\u2019s architecture. Learn about its enhancements over traditional BERT, use cases, and performance metrics.</p>"},{"location":"overview/learn_more/#4-modernbert-time-for-a-new-bert","title":"4. ModernBERT: Time for a New BERT","text":"<p>A thought-provoking discussion on why BERT models need an upgrade, showcasing how ModernBERT meets evolving NLP demands.</p>"},{"location":"overview/learn_more/#additional-citations","title":"\ud83d\udcd6 Additional Citations","text":"<ul> <li>ModernBERT-QnA: Fine-Tuned for SQuAD</li> </ul>"},{"location":"overview/learn_more/#summary","title":"\u2728 Summary","text":"<p>These resources provide comprehensive insights into ModernBERT\u2019s capabilities, practical implementations, and industry use cases. Whether you're a researcher, developer, or enthusiast, these links will guide you in exploring the full potential of ModernBERT. \ud83d\ude80</p>"}]}